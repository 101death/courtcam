#!/usr/bin/env python3
"""
Tennis Court Detector - Detects people on tennis courts using YOLOv5
"""
import os
import cv2
import numpy as np
import json
import torch
from shapely.geometry import Polygon, Point
import sys
import ssl
import argparse
import time
import io
from datetime import datetime
from contextlib import redirect_stdout, redirect_stderr
import contextlib
import threading
from multiprocessing import cpu_count, Pool
from functools import partial
import urllib.request
import re
import urllib.error
import urllib.parse
import importlib.util
import traceback
from collections import Counter
import logging
import subprocess
import math
import shutil

# === CONFIGURATION SETTINGS ===
class Config:
    # Color settings for court detection
    COURT_COLORS = {
        "blue": {
            "lower": [90, 40, 40],
            "upper": [120, 255, 255]
        },
        "green": {
            "lower": [40, 40, 40],
            "upper": [80, 255, 255]
        },
        "red": {
            "lower": [0, 50, 50],
            "upper": [10, 255, 255],
            "lower2": [170, 50, 50],
            "upper2": [180, 255, 255]
        }
    }
    
    # Court detection parameters
    class Court:
        MIN_AREA = 3000              # Reduced from 5000 to detect smaller courts
        MAX_AREA = 200000            # Increased from 150000 to detect larger courts
        MIN_SCORE = 0.5              # Minimum score for a valid court
        MIN_ASPECT_RATIO = 1.0       # Reduced from 1.2 to allow more court shapes
        MAX_ASPECT_RATIO = 4.0       # Increased from 3.0 to allow wider courts
        MIN_BLUE_RATIO = 0.2         # Reduced from 0.3 to be more lenient
        MIN_GREEN_RATIO = 0.02       # Reduced from 0.05 to be more lenient
    
    # Morphological operation settings
    class Morphology:
        KERNEL_SIZE = 5              # Kernel size for morphological operations
        ITERATIONS = 2               # Number of iterations for closing operations
    
    # Court area definitions
    IN_BOUNDS_COLOR = "blue"         # Color that represents in-bounds
    OUT_BOUNDS_COLOR = "green"       # Color that represents out-of-bounds
    
    # Visualization settings
    class Visual:
        COURT_OUTLINE_COLOR = (0, 255, 0)        # Green
        COURT_OUTLINE_THICKNESS = 4              # Line thickness
        PERSON_IN_BOUNDS_COLOR = (0, 255, 0)     # Green for people in court
        PERSON_OUT_BOUNDS_COLOR = (0, 165, 255)  # Orange for people near court
        PERSON_OFF_COURT_COLOR = (0, 0, 255)     # Red for people off court
        TEXT_COLOR = (255, 255, 255)             # White
        FONT_SCALE = 0.5                         # Text size
        TEXT_THICKNESS = 2                       # Text thickness
        DRAW_COURT_OUTLINE = True                # Whether to draw court outline
        SHOW_COURT_NUMBER = True                 # Whether to show court number in labels
        SHOW_DETAILED_LABELS = True              # Whether to show detailed labels on output image
    
    # Terminal output settings
    class Output:
        VERBOSE = False              # Default to non-verbose output
        USE_COLOR_OUTPUT = True      # Use colored terminal output
        SHOW_TIMESTAMP = True        # Show timestamps in output
        SUPER_QUIET = False          # Super quiet mode (almost no output)
        SUMMARY_ONLY = False         # Only show summary of results
        EXTRA_VERBOSE = False        # Show extra detailed output for Raspberry Pi
        
        # ANSI color codes for terminal output
        COLORS = {
            "INFO": "\033[94m",      # Blue
            "SUCCESS": "\033[92m",   # Green
            "WARNING": "\033[93m",   # Yellow
            "ERROR": "\033[91m",     # Red
            "DEBUG": "\033[90m",     # Gray
            "RESET": "\033[0m",      # Reset
            "BOLD": "\033[1m",       # Bold
            "UNDERLINE": "\033[4m"   # Underline
        }
    
    # Debug mode
    DEBUG_MODE = False              # Detailed debug output mode
    
    # Paths and directories
    class Paths:
        IMAGES_DIR = "images"
        INPUT_IMAGE = "input.png"
        OUTPUT_IMAGE = "output.png"
        MODELS_DIR = "models"
        OUTPUT_DIR = "images"
        
        @classmethod
        def input_path(cls):
            return os.path.join(cls.IMAGES_DIR, cls.INPUT_IMAGE)
            
        @classmethod
        def output_path(cls):
            return os.path.join(cls.IMAGES_DIR, cls.OUTPUT_IMAGE)
            
        @classmethod
        def debug_dir(cls):
            return os.path.join(os.path.dirname(cls.output_path()), "debug")
    
    # Model settings
    class Model:
        NAME = "yolov8n"             # YOLOv5 model size (yolov5s, yolov5m, yolov5l, etc.)
        CONFIDENCE = 0.25            # Detection confidence threshold
        IOU = 0.45                   # IoU threshold
        CLASSES = [0]                # Only detect people (class 0)
        
        # YOLOv5 model URLs - add more as needed
        MODEL_URLS = {
            "yolov5n": "https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5n.pt",
            "yolov5s": "https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt",
            "yolov5m": "https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m.pt",
            "yolov5l": "https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5l.pt",
            "yolov5x": "https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5x.pt",
            # YOLOv6 models
            "yolov6n": "https://github.com/meituan/YOLOv6/releases/download/0.2.0/yolov6n.pt",
            "yolov6s": "https://github.com/meituan/YOLOv6/releases/download/0.2.0/yolov6s.pt",
            "yolov6m": "https://github.com/meituan/YOLOv6/releases/download/0.2.0/yolov6m.pt",
            "yolov6l": "https://github.com/meituan/YOLOv6/releases/download/0.2.0/yolov6l.pt",
            # YOLOv7 models
            "yolov7": "https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt",
            "yolov7-tiny": "https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt",
            "yolov7x": "https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt",
            # YOLOv8 models
            "yolov8n": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt",
            "yolov8s": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt",
            "yolov8m": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt",
            "yolov8l": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt",
            "yolov8x": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt",
            # YOLOv9 models (added for future compatibility)
            "yolov9c": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9c.pt",
            "yolov9e": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9e.pt",
            "yolov9m": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9m.pt",
            "yolov9s": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9s.pt",
            "yolov9n": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9n.pt",
            "yolov9x": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov9x.pt",
            # YOLOv10 models (added for future compatibility)
            "yolov10n": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov10n.pt",
            "yolov10s": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov10s.pt",
            "yolov10m": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov10m.pt",
            "yolov10l": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov10l.pt",
            "yolov10x": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov10x.pt",
            # YOLOv11 models (placeholder for future versions)
            "yolov11n": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11n.pt",
            "yolov11s": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11s.pt",
            "yolov11m": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11m.pt",
            "yolov11l": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11l.pt",
            "yolov11x": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11x.pt",
            # YOLOv12 models
            "yolov12n": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov12n.pt",
            "yolov12s": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov12s.pt",
            "yolov12m": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov12m.pt",
            "yolov12l": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov12l.pt",
            "yolov12x": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov12x.pt",
        }
        
        @classmethod
        def get_model_url(cls, model_name):
            """Get URL for a YOLO model, including newer versions not explicitly listed."""
            model_name = model_name.lower()
            
            # Check if model exists directly in our dictionary
            if model_name in cls.MODEL_URLS:
                return cls.MODEL_URLS[model_name]
            
            # Handle dynamically newer YOLO versions (v9-v20)
            # Extract version and size from model name
            if model_name.startswith("yolov"):
                try:
                    # Extract version number and size
                    version_match = re.search(r'yolov(\d+)([a-z\-]+)?', model_name)
                    if version_match:
                        version = version_match.group(1)
                        size = version_match.group(2) or "s"  # Default to small if no size specified
                        
                        # If it's version 9 or higher, use the ultralytics assets pattern
                        if int(version) >= 9:
                            return f"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov{version}{size}.pt"
                except:
                    pass  # If parsing fails, fall back to default
            
            # Default to yolov5s if model not found
            OutputManager.log(f"Model {model_name} not found in known models. Defaulting to yolov5s.", "WARNING")
            return cls.MODEL_URLS["yolov5s"]
        
        AVAILABLE_MODELS = [
            "yolov5n",  # nano
            "yolov5s",  # small
            "yolov5m",  # medium
            "yolov5l",  # large
            "yolov5x",  # xlarge
            "yolov8n",  # nano
            "yolov8s",  # small
            "yolov8m",  # medium
            "yolov8l",  # large
            "yolov8x",  # xlarge
        ]
    
    # Multiprocessing settings
    class MultiProcessing:
        ENABLED = True              
        NUM_PROCESSES = max(1, cpu_count() - 1)  # Use all cores except one
        CHUNK_SIZE = 125             # Chunk size for processing

# Default lower resolution for better performance on Raspberry Pi
DEFAULT_CAMERA_RESOLUTION = (640, 480)  # Reduced resolution for better performance

# Global variables
CAMERA_AVAILABLE = False

# Fix macOS SSL certificate issues
if sys.platform == 'darwin':
    # Check if we're running on macOS and set default SSL context
    try:
        # Try to import certifi for better certificate handling
        import certifi
        ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())
    except ImportError:
        # If certifi isn't available, try the default macOS certificate location
        ssl._create_default_https_context = lambda: ssl.create_default_context()

# Check if ultralytics is installed for YOLOv8 models
try:
    import ultralytics
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

# Try to import camera module
try:
    from camera import takePhoto, DEFAULT_RESOLUTION, CameraOutputFormatter
    CAMERA_AVAILABLE = True
    DEFAULT_CAMERA_RESOLUTION = DEFAULT_RESOLUTION
except ImportError:
    # Define dummy takePhoto function when camera is not available
    def takePhoto(resolution=DEFAULT_CAMERA_RESOLUTION, output_file='images/input.png'):
        return False
    
    # Define dummy CameraOutputFormatter class when camera module is not available
    class CameraOutputFormatter:
        def __enter__(self):
            return self
        def __exit__(self, exc_type, exc_val, exc_tb):
            pass

# Global variables
args = None  # Will store command-line arguments

# Output manager for centralized output handling
class OutputManager:
    """
    Centralized output manager for all terminal output.
    Provides professional formatting, reliable animations, and clean output management.
    """
    # ANSI color and style codes
    BLUE = "\033[94m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    GRAY = "\033[90m"
    CYAN = "\033[96m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"
    RESET = "\033[0m"
    
    # Symbols for different message types
    SYMBOLS = {
        "INFO": "ℹ",
        "SUCCESS": "✓",
        "WARNING": "⚠",
        "ERROR": "✗",
        "DEBUG": "•",
        "FATAL": "☠",
        "STATUS": "→",
    }
    
    # Track messages for summary
    warnings = []
    errors = []
    successes = []
    info = []
    
    # Animation state
    _animation_active = False
    _animation_thread = None
    _stop_animation = False
    
    @classmethod
    def reset_logs(cls):
        """Reset all tracked messages"""
        cls.warnings = []
        cls.errors = []
        cls.successes = []
        cls.info = []
    
    @classmethod
    def _ensure_animation_stopped(cls):
        """Ensures any animation is stopped before printing new output"""
        if cls._animation_active:
            sys.stdout.write("\r\033[K")
            sys.stdout.flush()
            cls._animation_active = False
            cls._stop_animation = True 
            if cls._animation_thread and cls._animation_thread.is_alive():
                cls._animation_thread.join(timeout=0.1)

    @classmethod
    def log(cls, message, level="INFO"):
        """Logs messages, heavily filtering based on verbosity flags."""
        
        # Always track errors and warnings for the summary
        if level == "WARNING":
            cls.warnings.append(message)
        elif level == "ERROR" or level == "FATAL":
            cls.errors.append(message)
            # Print only the concise error indicator immediately
            cls._print_concise_error_indicator()
            return # Stop further processing for errors
        elif level == "SUCCESS":
            cls.successes.append(message)
        elif level == "INFO":
            cls.info.append(message)

        # Determine if the message should be printed based on level and flags
        should_print = False
        if Config.Output.SUPER_QUIET:
            # Super quiet: Only print final success or fatal errors
            if level == "SUCCESS" and ("saved" in message.lower() or "completed" in message.lower()):
                should_print = True
        elif Config.Output.EXTRA_VERBOSE:
            should_print = True # Print everything if extra verbose
        elif Config.Output.VERBOSE:
            # Verbose: Print INFO, STATUS, SUCCESS, WARNING
            if level in ["INFO", "STATUS", "SUCCESS", "WARNING"]:
                should_print = True
        else:
            # Default (non-verbose): Print only key SUCCESS messages
            if level == "SUCCESS":
                # Only print specific success messages deemed important
                if any(keyword in message.lower() for keyword in ["found", "detected", "saved", "loaded", "completed", "captured"]):
                    should_print = True
        
        # Exit if message shouldn't be printed
        if not should_print:
            return

        # Stop any residual animation (safety check)
        cls._ensure_animation_stopped()

        # --- Formatting and Printing --- 
        color = ""
        symbol = ""
        
        if Config.Output.USE_COLOR_OUTPUT:
            color_map = {"INFO": cls.BLUE, "SUCCESS": cls.GREEN, "WARNING": cls.YELLOW, 
                        "DEBUG": cls.GRAY, "STATUS": cls.CYAN, "ERROR": cls.RED, "FATAL": cls.RED}
            color = color_map.get(level, "")

        symbol = cls.SYMBOLS.get(level, "")
        try:
            if symbol and sys.stdout.encoding is not None:
                symbol.encode(sys.stdout.encoding)
            else:
                # Use ASCII fallback if encoding is None or symbol fails
                ascii_map = {"INFO": "i", "SUCCESS": "+", "WARNING": "!", "DEBUG": ".", "STATUS": ">", "ERROR": "x", "FATAL": "!"}
                symbol = ascii_map.get(level, "")
        except (UnicodeEncodeError, AttributeError):
            ascii_map = {"INFO": "i", "SUCCESS": "+", "WARNING": "!", "DEBUG": ".", "STATUS": ">", "ERROR": "x", "FATAL": "!"}
            symbol = ascii_map.get(level, "")

        timestamp = ""
        if Config.Output.SHOW_TIMESTAMP:
            timestamp = f"{cls.GRAY}[{datetime.now().strftime('%H:%M:%S')}]{cls.RESET} "
            
        formatted_message = f"{timestamp}{color}{symbol} {message}{cls.RESET}"
        print(formatted_message)
        sys.stdout.flush()

    @classmethod
    def _print_concise_error_indicator(cls):
        """Prints a minimal error message to the console."""
        cls._ensure_animation_stopped()
        timestamp = ""
        if Config.Output.SHOW_TIMESTAMP:
            timestamp = f"{cls.GRAY}[{datetime.now().strftime('%H:%M:%S')}]{cls.RESET} "
        error_symbol = "✗"
        try:
            if sys.stdout.encoding is None:
                error_symbol = "x"
            else:
                error_symbol.encode(sys.stdout.encoding)
        except (UnicodeEncodeError, AttributeError):
            error_symbol = "x"
        print(f"{timestamp}{cls.RED}{error_symbol} Error occurred (details in summary){cls.RESET}")
        sys.stdout.flush()

    @classmethod
    def status(cls, message):
        """Log a status update message - only if verbose."""
        # Status messages only shown in verbose modes
        if Config.Output.VERBOSE or Config.Output.EXTRA_VERBOSE:
            cls.log(message, "STATUS")
    
    @classmethod
    def stop_animation(cls, success=True):
        """Ensures any lingering animation state is cleared."""
        cls._ensure_animation_stopped()

    @classmethod
    def get_potential_fixes(cls):
        """Returns potential fixes for common errors"""
        if not cls.errors:
            return None
        
        fixes = []
        
        # Look for common error patterns and suggest fixes
        for error in cls.errors:
            error_str = str(error).lower()
            
            if "no module named 'picamera'" in error_str:
                fixes.append("1. To install picamera module: sudo apt install python3-picamera")
                fixes.append("2. Or run with --no-camera flag to skip camera functionality")
            
            elif "no module named 'numpy'" in error_str or "numpy" in error_str and "version" in error_str:
                fixes.append("1. Install compatible NumPy: pip install 'numpy==1.19.5'")
                fixes.append("2. For Raspberry Pi, try: pip install 'numpy==1.19.5' --only-binary=:all:")
            
            elif "no module named 'opencv'" in error_str or "cv2" in error_str:
                fixes.append("1. Install OpenCV: pip install opencv-python")
                fixes.append("2. For Raspberry Pi: sudo apt install python3-opencv")
            
            elif "permission" in error_str:
                fixes.append("Try running with sudo or check file/folder permissions")
                
            elif "no such file" in error_str:
                fixes.append("Check file path and ensure required files exist")
                
            elif "timeout" in error_str or "connection" in error_str:
                fixes.append("Check network connection or try again later")
                
            elif "memory" in error_str:
                fixes.append("1. Free up system memory or reduce image resolution")
                fixes.append("2. Try running with --low-memory flag")
        
        if not fixes:
            fixes = ["1. Check command-line arguments",
                    "2. Ensure required files and dependencies are available",
                    "3. Run with --verbose flag for more detailed output"]
        
        return "\n".join(fixes)

    @classmethod
    def summarize_detections(cls, courts, people, people_locations):
        """Summarize detection results in a more concise format"""
        if Config.Output.SUPER_QUIET:
            return {}
            
        # Only print detection summary in verbose mode
        if Config.Output.VERBOSE or Config.Output.EXTRA_VERBOSE:
            cls.log(f"{cls.BOLD}Detection Summary:{cls.RESET}", "INFO")
            cls.log(f"Found {len(courts)} tennis courts", "SUCCESS")
            cls.log(f"Found {len(people)} {'person' if len(people) == 1 else 'people'} in the image", "SUCCESS")
        
        # Count people by court
        court_counts = {}
        for court_idx, area_type in people_locations:
            if court_idx >= 0:
                court_num = court_idx + 1
                if court_num not in court_counts:
                    court_counts[court_num] = 0
                court_counts[court_num] += 1
        
        # Print court-specific counts only in verbose mode
        if Config.Output.VERBOSE or Config.Output.EXTRA_VERBOSE:
            for court_num in sorted(court_counts.keys()):
                cls.log(f"Court {court_num}: {court_counts[court_num]} {'person' if court_counts[court_num] == 1 else 'people'}", "INFO")
        
        return court_counts
    
    @classmethod
    def create_final_summary(cls, people_count, court_counts, output_path=None, total_courts=None):
        """Create a concise final summary, incorporating stored errors/warnings."""
        summary_lines = []

        # Core result summary
        if not cls.errors:
            if people_count is not None:
                people_text = f"{people_count} person{'s' if people_count != 1 else ''}"
                court_text = f"{total_courts} court{'s' if total_courts != 1 else ''}" if total_courts is not None else "No courts"
                
                summary_lines.append(f"{cls.BOLD}{people_text}{cls.RESET} detected on {cls.BOLD}{court_text}{cls.RESET}")

                # Add court details if people were found on courts
                if court_counts:
                    court_details = []
                    for court_num, count in sorted(court_counts.items()):
                        court_details.append(f"C{court_num}: {count}")
                    summary_lines.append(f"Distribution: {', '.join(court_details)}")
            else: 
                summary_lines.append(f"{cls.BOLD}No detections{cls.RESET}")
        else:
            # Error summary
            summary_lines.append(f"{cls.RED}{cls.BOLD}Processing failed{cls.RESET}")

        # Add output path if generated
        if output_path and not cls.errors:
            summary_lines.append(f"Output: {os.path.basename(output_path)}")

        # Add Errors Section
        if cls.errors:
            summary_lines.append("") 
            summary_lines.append(f"{cls.RED}ERRORS:{cls.RESET}")
            for i, error in enumerate(cls.errors, 1):
                error_short = str(error).split('\n')[0]  # Get first line
                summary_lines.append(f" {i}. {error_short[:70]}{'...' if len(error_short) > 70 else ''}")
            
            # Add troubleshooting suggestions if there are errors
            error_fixes = cls.get_potential_fixes()
            if error_fixes:
                summary_lines.append("")
                summary_lines.append(f"{cls.YELLOW}TROUBLESHOOTING:{cls.RESET}")
                # Add fixes line by line for wrapping
                for line in error_fixes.split('\n'):
                    summary_lines.append(line)
        
        # Add Warnings Section (only if no errors)
        elif cls.warnings:
            summary_lines.append("") 
            summary_lines.append(f"{cls.YELLOW}WARNINGS:{cls.RESET}")
            for i, warning in enumerate(cls.warnings, 1):
                summary_lines.append(f" {i}. {warning}")

        return "\n".join(summary_lines)

    @classmethod
    def fancy_summary(cls, title, content, processing_time=None):
        """
        Display a fancy boxed summary with consistent formatting
        
        Args:
            title: Title of the summary box
            content: List of content lines or a string with newlines
            processing_time: Optional processing time to display
        """
        # Stop any running animation
        cls._ensure_animation_stopped()
        
        # Helper function to wrap text by words
        def wrap_text(text, width):
            # First remove any ANSI color codes for length calculation
            ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
            clean_text = ansi_escape.sub('', text)
            
            words = text.split()
            lines = []
            current_line = []
            current_length = 0
            
            for word in words:
                # Calculate word length without ANSI codes
                clean_word = ansi_escape.sub('', word)
                word_length = len(clean_word)
                
                if current_length + word_length + (1 if current_length > 0 else 0) <= width:
                    # Add word to current line
                    if current_length > 0:
                        current_length += 1  # For the space
                        current_line.append(" ")
                    current_line.append(word)
                    current_length += word_length
                else:
                    # Line is full, start a new one
                    if current_line:
                        lines.append("".join(current_line))
                    current_line = [word]
                    current_length = word_length
            
            # Add the last line if it exists
            if current_line:
                lines.append("".join(current_line))
            
            return lines
        
        # Process the content
        max_width = 78  # Maximum characters per line
        content_lines = []
        
        # Check if string contains ANSI escape sequences
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        
        if isinstance(content, str):
            # Split the content by newlines first
            for paragraph in content.split('\n'):
                if not paragraph:
                    # Keep empty lines
                    content_lines.append("")
                else:
                    # Wrap the paragraph
                    wrapped_lines = wrap_text(paragraph, max_width)
                    content_lines.extend(wrapped_lines)
        else:
            # Handle list input
            for paragraph in content:
                if not paragraph:
                    content_lines.append("")
                else:
                    wrapped_lines = wrap_text(paragraph, max_width)
                    content_lines.extend(wrapped_lines)
        
        # Add processing time if provided
        if processing_time is not None:
            content_lines.append("")
            content_lines.append(f"Processing time: {processing_time:.2f} seconds")
        
        # Calculate box width based on content, accounting for ANSI escape codes
        def visual_length(s):
            return len(ansi_escape.sub('', s))
            
        content_width = max(visual_length(line) for line in content_lines) if content_lines else 0
        box_width = max(content_width + 4, visual_length(title) + 10, 60)
        
        # Try to detect terminal width
        try:
            terminal_width, _ = os.get_terminal_size()
            # Limit box width to terminal width - 2
            box_width = min(box_width, terminal_width - 2)
        except (AttributeError, OSError):
            # If we can't get terminal width, use default
            pass
        
        # Choose box drawing characters based on terminal capability
        use_unicode_box = True
        try:
            # Test if terminal can display Unicode box drawing characters
            if sys.stdout.encoding is not None:
                "│".encode(sys.stdout.encoding)
            else:
                use_unicode_box = False
        except (UnicodeEncodeError, AttributeError):
            use_unicode_box = False
            
        if use_unicode_box:
            # Box drawing characters (Unicode)
            top_left = "╭"
            top_right = "╮"
            bottom_left = "╰"
            bottom_right = "╯"
            horizontal = "─"
            vertical = "│"
            t_right = "├"
            t_left = "┤"
        else:
            # Fallback to ASCII box characters
            top_left = "+"
            top_right = "+"
            bottom_left = "+"
            bottom_right = "+"
            horizontal = "-"
            vertical = "|"
            t_right = "+"
            t_left = "+"
        
        # Build the box parts
        top_border = f"{top_left}{horizontal * (box_width - 2)}{top_right}"
        title_line = f"{vertical} {cls.BOLD}{title.center(box_width - 4)}{cls.RESET} {vertical}"
        divider = f"{t_right}{horizontal * (box_width - 2)}{t_left}"
        empty_line = f"{vertical}{' ' * (box_width - 2)}{vertical}"
        bottom_border = f"{bottom_left}{horizontal * (box_width - 2)}{bottom_right}"
        
        # Display the summary box
        # First clear any existing content
        sys.stdout.write("\r\033[K")
        sys.stdout.flush()
        
        print(top_border)
        print(title_line)
        print(divider)
        print(empty_line)
        
        # Print content with proper handling of ANSI escape codes
        for line in content_lines:
            # Get visual length of the line (excluding ANSI codes)
            vis_len = visual_length(line)
            # Calculate padding needed
            padding = box_width - 4 - vis_len
            # Format the line with proper padding
            if padding >= 0:
                print(f"{vertical} {line}{' ' * padding} {vertical}")
            else:
                # Line is too long, truncate
                print(f"{vertical} {line[:box_width - 4]} {vertical}")
        
        print(empty_line)
        print(bottom_border)
        
        # Ensure output is displayed immediately
        sys.stdout.flush()

@contextlib.contextmanager
def suppress_stdout_stderr():
    """Context manager to suppress stdout and stderr output"""
    # Save the original stdout and stderr
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    
    # Create string buffers to capture output
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()
    
    # Redirect output to the buffers
    sys.stdout = stdout_buffer
    sys.stderr = stderr_buffer
    
    try:
        yield  # Code inside the with block runs with redirected stdout/stderr
    finally:
        # Restore the original stdout and stderr
        sys.stdout = old_stdout
        sys.stderr = old_stderr

def create_blue_mask(image):
    """Create a mask for blue areas in the image"""
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # Get blue range from config
    blue_range = Config.COURT_COLORS["blue"]
    lower = np.array(blue_range["lower"])
    upper = np.array(blue_range["upper"])
    
    # Create mask
    blue_mask = cv2.inRange(hsv, lower, upper)
    
    # Clean up mask
    kernel = np.ones((Config.Morphology.KERNEL_SIZE, Config.Morphology.KERNEL_SIZE), np.uint8)
    blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_OPEN, kernel)
    blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kernel, iterations=Config.Morphology.ITERATIONS)
    
    return blue_mask

def create_green_mask(image):
    """Create a mask for green areas in the image"""
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # Get green range from config
    green_range = Config.COURT_COLORS["green"]
    lower = np.array(green_range["lower"])
    upper = np.array(green_range["upper"])
    
    # Create mask
    green_mask = cv2.inRange(hsv, lower, upper)
    
    # Clean up mask
    kernel = np.ones((Config.Morphology.KERNEL_SIZE, Config.Morphology.KERNEL_SIZE), np.uint8)
    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)
    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_CLOSE, kernel, iterations=Config.Morphology.ITERATIONS)
    
    return green_mask

def is_sky_region(contour, image_height, image_width):
    """Check if a contour is likely to be sky based on position and characteristics"""
    x, y, w, h = cv2.boundingRect(contour)
    
    # Sky is usually at the top of the image
    is_at_top = y < image_height * 0.15
    
    # Sky is usually wide
    is_wide = w > image_width * 0.5
    
    # Sky usually has a small height
    is_short = h < image_height * 0.2
    
    # Check if the contour is likely to be sky
    return is_at_top and (is_wide or is_short)

def process_court_contour(contour, blue_mask, green_mask, height, width):
    """Process a single court contour - designed for multiprocessing"""
    area = cv2.contourArea(contour)
    # Filter by minimum area to avoid noise
    if area < Config.Court.MIN_AREA:
        return None
        
    # Get bounding box for aspect ratio check
    x, y, w, h = cv2.boundingRect(contour)
    
    # Create slightly dilated mask for this blue region to check if it's next to green
    region_mask = np.zeros((height, width), dtype=np.uint8)
    cv2.drawContours(region_mask, [contour], -1, 255, -1)
    
    # Dilate the region mask slightly to check for adjacent green pixels
    kernel = np.ones((15, 15), np.uint8)
    dilated_region = cv2.dilate(region_mask, kernel, iterations=1)
    
    # Check if there's green adjacent to this blue region
    green_nearby = cv2.bitwise_and(green_mask, dilated_region)
    green_nearby_pixels = cv2.countNonZero(green_nearby)
    
    # If there's no green nearby, it's not a court
    if green_nearby_pixels < 100:  # Threshold for minimum green pixels needed
        return None
    
    # Get the blue area itself
    blue_area = cv2.bitwise_and(blue_mask, region_mask)
    blue_pixels = cv2.countNonZero(blue_area)
    
    # Get convex hull for better shape
    hull = cv2.convexHull(contour)
    
    # Approximate polygon
    perimeter = cv2.arcLength(hull, True)
    approx = cv2.approxPolyDP(hull, 0.02 * perimeter, True)
    
    # Calculate centroid
    M = cv2.moments(contour)
    if M["m00"] != 0:
        cx = int(M["m10"] / M["m00"])
        cy = int(M["m01"] / M["m00"])
    else:
        cx, cy = x + w//2, y + h//2
    
    # Save court info with all required keys
    court_info = {
        'contour': contour,
        'approx': approx,
        'hull': hull,
        'area': area,
        'blue_ratio': 1.0,  # This is a blue region, so ratio is 1
        'green_ratio': green_nearby_pixels / area,
        'blue_mask': blue_area,
        'green_mask': green_nearby,
        'blue_pixels': blue_pixels,
        'green_pixels': green_nearby_pixels,
        'centroid': (cx, cy),
        'bbox': (x, y, w, h)
    }
    
    return court_info

def check_person_on_court(person_data):
    """
    Process a single person to determine court position - designed for multiprocessing
    person_data: tuple of (person, courts)
    Returns: (court_index, area_type)
    """
    person, courts = person_data
    
    # Get the bounding box coordinates
    x1, y1, x2, y2 = person['bbox']
    
    # Calculate the bottom half of the bounding box
    bottom_y1 = y1 + (y2 - y1) // 2  # Start from middle of box
    bottom_y2 = y2  # End at bottom of box
    
    # Create points for the bottom half of the bounding box
    bottom_points = [
        Point(x1, bottom_y1),
        Point(x2, bottom_y1),
        Point(x2, bottom_y2),
        Point(x1, bottom_y2)
    ]
    
    # Check each court
    for court_idx, court in enumerate(courts):
        # Get the court polygon
        approx = court['approx']
        points = approx.reshape(-1, 2)
        court_polygon = Polygon(points)
        
        # Check if any of the bottom points are inside the court
        for point in bottom_points:
            if court_polygon.contains(point):
                # Person is on this court - now determine if they're on blue (in-bounds) or green (out-bounds)
                x, y = int(point.x), int(point.y)
                
                # Check if the point is on blue area (in-bounds)
                blue_mask = court['blue_mask']
                if y < blue_mask.shape[0] and x < blue_mask.shape[1] and blue_mask[y, x] > 0:
                    return court_idx, 'in_bounds'
                
                # Check if the point is on green area (out-bounds)
                green_mask = court['green_mask']
                if y < green_mask.shape[0] and x < green_mask.shape[1] and green_mask[y, x] > 0:
                    return court_idx, 'out_bounds'
                
                # If not specifically on blue or green, consider it in-bounds if the court has more blue than green
                if court['blue_ratio'] > court['green_ratio']:
                    return court_idx, 'in_bounds'
                else:
                    return court_idx, 'out_bounds'
    
    # If we reached here, the person is not on any court
    return -1, 'off_court'

def process_courts_parallel(blue_contours, blue_mask, green_mask, height, width):
    """Process court contours in parallel using multiprocessing"""
    if not Config.MultiProcessing.ENABLED or len(blue_contours) <= 5:
        # For few contours, sequential processing is faster
        courts = []
        for contour in blue_contours:
            court_info = process_court_contour(contour, blue_mask, green_mask, height, width)
            if court_info:
                courts.append(court_info)
                OutputManager.log(f"Court {len(courts)} accepted: Area={court_info['area']:.1f}, Green nearby pixels={court_info['green_pixels']}", "SUCCESS")
        return courts
    
    # For many contours, use multiprocessing
    OutputManager.log(f"Processing {len(blue_contours)} potential courts with {Config.MultiProcessing.NUM_PROCESSES} processes", "INFO")
    
    # Create a partial function with fixed arguments
    process_func = partial(process_court_contour, blue_mask=blue_mask, green_mask=green_mask, 
                          height=height, width=width)
    
    # Create a pool and process contours in parallel
    with Pool(processes=Config.MultiProcessing.NUM_PROCESSES) as pool:
        results = pool.map(process_func, blue_contours)
    
    # Filter None results and collect valid courts
    courts = [court for court in results if court is not None]
    
    # Log the results
    for i, court in enumerate(courts):
        OutputManager.log(f"Court {i+1} accepted: Area={court['area']:.1f}, Green nearby pixels={court['green_pixels']}", "SUCCESS")
    
    return courts

def analyze_people_positions_parallel(people, courts):
    """Analyze positions of people on courts using multiprocessing"""
    if not Config.MultiProcessing.ENABLED or len(people) <= 5:
        # For few people, sequential processing is faster
        people_locations = []
        for person in people:
            court_idx, area_type = is_person_on_court(person, courts)
            people_locations.append((court_idx, area_type))
        return people_locations
    
    # For many people, use multiprocessing
    OutputManager.log(f"Analyzing positions of {len(people)} people with {Config.MultiProcessing.NUM_PROCESSES} processes", "INFO")
    
    # Create input data for the pool
    input_data = [(person, courts) for person in people]
    
    # Create a pool and process positions in parallel
    with Pool(processes=Config.MultiProcessing.NUM_PROCESSES) as pool:
        people_locations = pool.map(check_person_on_court, input_data)
    
    return people_locations

def detect_tennis_court(image, debug_folder=None):
    """
    Detect tennis courts in an image using color masking and contour analysis.
    Simplified approach: Every blue area next to green is a court.
    Returns list of tennis court contours.
    
    Optimized for Raspberry Pi Zero with multiprocessing support.
    """
    height, width = image.shape[:2]
    
    # Create masks
    OutputManager.status("Creating blue and green masks...")
    blue_mask = create_blue_mask(image)
    green_mask = create_green_mask(image)
    
    # Save raw masks for debugging
    if debug_folder and Config.DEBUG_MODE and Config.Output.VERBOSE:
        cv2.imwrite(os.path.join(debug_folder, "blue_mask_raw.png"), blue_mask)
        cv2.imwrite(os.path.join(debug_folder, "green_mask_raw.png"), green_mask)
    
    # Find blue contours (potential courts)
    OutputManager.status("Analyzing potential court shapes...")
    blue_contours, _ = cv2.findContours(blue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Process contours in parallel
    valid_courts = process_courts_parallel(blue_contours, blue_mask, green_mask, height, width)
    
    # Save debug visualizations
    if debug_folder and Config.DEBUG_MODE and Config.Output.VERBOSE:
        # Create visualization of masks
        masks_viz = np.zeros((height, width, 3), dtype=np.uint8)
        masks_viz[blue_mask > 0] = [255, 0, 0]  # Blue
        masks_viz[green_mask > 0] = [0, 255, 0]  # Green
        cv2.imwrite(os.path.join(debug_folder, "color_masks.png"), masks_viz)
        
        # Create visualization of all courts
        if valid_courts:
            courts_viz = image.copy()
            for i, court in enumerate(valid_courts):
                # Draw court outline
                cv2.drawContours(courts_viz, [court['approx']], 0, Config.Visual.COURT_OUTLINE_COLOR, 2)
                
                # Add court number
                x, y, w, h = court['bbox']
                cv2.putText(courts_viz, f"Court {i+1}", (x + w//2 - 40, y + h//2),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
            
            cv2.imwrite(os.path.join(debug_folder, "courts_detected.png"), courts_viz)
    
    if valid_courts:
        OutputManager.log(f"Found {len(valid_courts)} tennis courts", "SUCCESS")
    else:
        OutputManager.log("No tennis courts detected", "WARNING")
    
    return valid_courts

def is_person_on_court(person, courts):
    """
    Determine if a person is on a tennis court.
    Returns (court_index, area_type) where area_type is 'in_bounds', 'out_bounds', or 'off_court'
    Uses bottom half of bounding box for more accurate placement.
    """
    # Get the bounding box coordinates
    x1, y1, x2, y2 = person['bbox']
    
    # Calculate the bottom half of the bounding box
    bottom_y1 = y1 + (y2 - y1) // 2  # Start from middle of box
    bottom_y2 = y2  # End at bottom of box
    
    # Create points for the bottom half of the bounding box
    bottom_points = [
        Point(x1, bottom_y1),
        Point(x2, bottom_y1),
        Point(x2, bottom_y2),
        Point(x1, bottom_y2)
    ]
    
    # Check each court
    for court_idx, court in enumerate(courts):
        # Get the court polygon
        approx = court['approx']
        points = approx.reshape(-1, 2)
        court_polygon = Polygon(points)
        
        # Check if any of the bottom points are inside the court
        for point in bottom_points:
            if court_polygon.contains(point):
                # Person is on this court - now determine if they're on blue (in-bounds) or green (out-bounds)
                x, y = int(point.x), int(point.y)
                
                # Check if the point is on blue area (in-bounds)
                blue_mask = court['blue_mask']
                if y < blue_mask.shape[0] and x < blue_mask.shape[1] and blue_mask[y, x] > 0:
                    return court_idx, 'in_bounds'
                
                # Check if the point is on green area (out-bounds)
                green_mask = court['green_mask']
                if y < green_mask.shape[0] and x < green_mask.shape[1] and green_mask[y, x] > 0:
                    return court_idx, 'out_bounds'
                
                # If not specifically on blue or green, consider it in-bounds if the court has more blue than green
                if court['blue_ratio'] > court['green_ratio']:
                    return court_idx, 'in_bounds'
                else:
                    return court_idx, 'out_bounds'
    
    # If we reached here, the person is not on any court
    return -1, 'off_court'

def assign_court_numbers(blue_mask_connected):
    """
    Assign court numbers by clustering blue regions
    Returns a labeled mask where each court has a unique number
    """
    # Find all connected components in the blue mask
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(blue_mask_connected, connectivity=8)
    
    # The first label (0) is the background, so we start from 1
    courts = []
    
    # Filter out small components (noise)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area >= Config.Court.MIN_AREA:
            x = stats[i, cv2.CC_STAT_LEFT]
            y = stats[i, cv2.CC_STAT_TOP]
            w = stats[i, cv2.CC_STAT_WIDTH]
            h = stats[i, cv2.CC_STAT_HEIGHT]
            
            # Create a mask for this court
            court_mask = (labels == i).astype(np.uint8) * 255
            
            # Find contours of the court
            contours, _ = cv2.findContours(court_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                contour = contours[0]  # Use the largest contour
                
                # Get convex hull for better shape
                hull = cv2.convexHull(contour)
                
                # Approximate polygon
                perimeter = cv2.arcLength(hull, True)
                approx = cv2.approxPolyDP(hull, 0.02 * perimeter, True)
                
                # Calculate centroid
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                else:
                    cx, cy = x + w//2, y + h//2
                
                courts.append({
                    'id': i,
                    'area': area,
                    'bbox': (x, y, w, h),
                    'centroid': centroids[i],
                    'contour': contour,
                    'approx': approx,
                    'hull': hull,
                    'blue_ratio': 1.0,  # This is a blue region
                    'green_ratio': 0.0,  # Will be updated later
                    'blue_mask': court_mask,
                    'green_mask': np.zeros_like(court_mask),  # Will be updated later
                    'blue_pixels': area,
                    'green_pixels': 0  # Will be updated later
                })
    
    # Sort courts by x-coordinate to assign numbers from left to right
    courts.sort(key=lambda c: c['centroid'][0])
    
    # Create a renumbered mask
    court_mask = np.zeros_like(blue_mask_connected, dtype=np.uint8)
    
    # Assign new court numbers (1, 2, 3, ...) to each court based on sorted order
    for i, court in enumerate(courts):
        court_id = i + 1  # Start numbering from 1
        court['court_number'] = court_id
        # Extract original label mask and assign new number
        court_region = (labels == court['id']).astype(np.uint8) * court_id
        court_mask = cv2.add(court_mask, court_region)
    
    return court_mask, courts

def detect_people_ultralytics(model, image, confidence=0.25):
    """
    Detect people using the ultralytics API directly.
    Returns a list of people with their bounding boxes and confidence scores.
    
    This function uses a completely separate codepath for YOLOv8+ models.
    """
    people = []
    
    try:
        # Check if ultralytics is available
        if not ULTRALYTICS_AVAILABLE:
            OutputManager.log("Ultralytics package not installed - required for YOLOv8+ models", "ERROR")
            OutputManager.log("Install with: pip install ultralytics", "INFO")
            return people
            
        # Run prediction with person class only
        results = model.predict(
            image, 
            conf=confidence,  # Confidence threshold
            classes=[0],      # Only detect people (class 0)
            verbose=False     # Don't print progress
        )
        
        # Process results - directly extract people
        if len(results) > 0 and hasattr(results[0], 'boxes'):
            boxes = results[0].boxes
            
            # Check if we found any boxes
            if len(boxes) > 0:
                OutputManager.log(f"YOLOv8 detected {len(boxes)} people", "SUCCESS")
                
                # Process each detection
                for box in boxes:
                    # Get class and confidence
                    cls = int(box.cls.item()) if hasattr(box, 'cls') else 0
                    if cls == 0:  # Person class
                        conf = float(box.conf.item()) if hasattr(box, 'conf') else 0.0
                        
                        # Get coordinates - handle different tensor formats
                        if hasattr(box, 'xyxy'):
                            # Get coordinates as numpy array
                            xyxy = box.xyxy[0].cpu().numpy()
                            x1, y1, x2, y2 = map(int, xyxy)
                            
                            # Calculate center point and foot position
                            center_x = (x1 + x2) // 2
                            center_y = (y1 + y2) // 2
                            foot_x = center_x
                            foot_y = y2  # Bottom of bounding box represents feet
                            
                            # Add to people list
                            people.append({
                                'position': (center_x, center_y),
                                'foot_position': (foot_x, foot_y),
                                'bbox': (x1, y1, x2, y2),
                                'confidence': conf
                            })
            else:
                OutputManager.log("YOLOv8 found no people in the image", "INFO")
    except Exception as e:
        OutputManager.log(f"Error in ultralytics detection: {str(e)}", "ERROR")
        if "AttributeError" in str(e) and "module 'torch.nn.modules.module'" in str(e):
            OutputManager.log("This seems to be a compatibility issue with PyTorch and ultralytics", "INFO")
            OutputManager.log("Try updating PyTorch: pip install -U torch torchvision", "INFO")
        elif "No module named 'ultralytics'" in str(e):
            OutputManager.log("Ultralytics package is not installed", "ERROR")
            OutputManager.log("Install with: pip install ultralytics", "INFO")
    
    # Log how many people we found
    if people:
        OutputManager.log(f"Ultralytics API found {len(people)} people", "SUCCESS")
    else:
        OutputManager.log("No people detected with ultralytics API", "INFO")
        
    return people

def try_import_camera():
    """
    Attempts to import a compatible camera module.
    Returns a tuple of (camera_module, is_legacy, error_message).
    """
    import_errors = []

    # First try to import picamera2 (for newer Raspberry Pi OS)
    try:
        import picamera2
        from picamera2 import Picamera2
        return (picamera2, False, None)
    except ImportError as e:
        import_errors.append(("picamera2", str(e)))
    
    # Then try legacy picamera (for older Raspberry Pi OS)
    try:
        import picamera
        from picamera.array import PiRGBArray
        return (picamera, True, None)
    except ImportError as e:
        import_errors.append(("picamera", str(e)))
    
    # If we got here, we couldn't import either camera module
    # Only show message if we're in verbose mode
    if Config.Output.VERBOSE:
        for module_name, error in import_errors:
            print(f"Could not import {module_name}: {error}")
        print("Camera functionality will be simulated.")
        print("\nAlternative options:")
        print("1. Run with --no-camera to skip camera functionality:")
        print("   python main.py --no-camera")
        print("2. Place an image manually in the images/ directory:")
        print("   mkdir -p images && cp your_image.jpg images/input.png")
    else:
        # In non-verbose mode, just add the warning to the OutputManager
        OutputManager.log("Camera modules not available - simulation mode active.", "WARNING")
    
    # Return None to indicate no camera modules are available
    return (None, False, "No compatible camera module found")

def main():
    """Main function optimized for Raspberry Pi Zero"""
    # Start timer
    start_time = time.time()
    
    # Take photo only if camera is available
    if CAMERA_AVAILABLE:
        try:
            # Check if camera resolution is specified
            camera_resolution = (1920, 1080)  # Default resolution
            if hasattr(args, 'camera_resolution') and args.camera_resolution:
                try:
                    width, height = args.camera_resolution.split(',')
                    camera_resolution = (int(width), int(height))
                    OutputManager.log(f"Using custom camera resolution: {camera_resolution}", "INFO")
                except Exception as e:
                    OutputManager.log(f"Invalid camera resolution format: {args.camera_resolution}. Using default.", "WARNING")
            
            takePhoto(resolution=camera_resolution) # take a photo through the camera
            OutputManager.log("Photo captured successfully.", "SUCCESS")
        except Exception as e:
            OutputManager.log(f"Error taking photo: {e}", "ERROR")
            OutputManager.log("Proceeding without capturing a new photo.", "WARNING")
    else:
        OutputManager.log("Skipping photo capture as camera is not available.", "WARNING")
    
    # Reset any previously tracked logs
    OutputManager.reset_logs()
    
    # Initialize multiprocessing if enabled
    if Config.MultiProcessing.ENABLED:
        # Set the number of processes if not already set
        if Config.MultiProcessing.NUM_PROCESSES <= 0:
            # Use 3 processes or the number of CPU cores if less than 3
            Config.MultiProcessing.NUM_PROCESSES = min(3, cpu_count())
        
        OutputManager.log(f"Multiprocessing enabled with {Config.MultiProcessing.NUM_PROCESSES} processes", "INFO")
    
    try:
        # Load image
        input_path = Config.Paths.input_path()
        try:
            # First ensure the images directory exists
            images_dir = os.path.dirname(input_path)
            if not os.path.exists(images_dir):
                try:
                    os.makedirs(images_dir, exist_ok=True)
                    OutputManager.log(f"Created images directory at {images_dir}", "INFO")
                except Exception as e:
                    OutputManager.log(f"Cannot create images directory: {str(e)}", "ERROR")
            
            # Load the image
            OutputManager.status("Loading image")
            image = cv2.imread(input_path)
            
            # Check image loaded successfully
            if image is not None:
                OutputManager.log(f"Image loaded successfully: {image.shape[1]}x{image.shape[0]} pixels", "SUCCESS")
                if Config.Output.EXTRA_VERBOSE:
                    OutputManager.log(f"Image type: {image.dtype}, channels: {image.shape[2]}", "INFO")
            else:
                OutputManager.log(f"Unable to open the image at {input_path}", "ERROR")
                # Show final summary with error and exit
                processing_time = time.time() - start_time
                final_summary = OutputManager.create_final_summary(
                    people_count=None, 
                    court_counts={}, 
                    output_path=None,
                    processing_time=processing_time,
                    total_courts=0
                )
                print_error_summary(final_summary)
                sys.exit(1)
        except Exception as e:
            OutputManager.log(f"Problem loading the image: {str(e)}", "ERROR")
            processing_time = time.time() - start_time
            final_summary = OutputManager.create_final_summary(
                people_count=None, 
                court_counts={}, 
                output_path=None,
                processing_time=processing_time,
                total_courts=0
            )
            print_error_summary(final_summary)
            sys.exit(1)
        
        # Set up debug folder
        try:
            if Config.DEBUG_MODE:
                debug_folder = Config.Paths.debug_dir()
                os.makedirs(debug_folder, exist_ok=True)
                OutputManager.log(f"Debug folder created at {debug_folder}", "DEBUG")
            else:
                debug_folder = None
        except Exception as e:
            OutputManager.log(f"Can't create debug folder: {str(e)}", "WARNING")
            debug_folder = None  # Set to None to prevent further debug saves
            # Continue execution even if debug folder can't be created
        
        # Detect tennis courts
        try:
            OutputManager.status("Analyzing court colors")
            blue_mask = create_blue_mask(image)
            green_mask = create_green_mask(image)
            OutputManager.log("Court colors analyzed", "SUCCESS")
            if Config.Output.EXTRA_VERBOSE:
                OutputManager.log(f"Blue mask: {np.count_nonzero(blue_mask)} pixels, Green mask: {np.count_nonzero(green_mask)} pixels", "INFO")
            
            # Process the raw blue mask to avoid connecting unrelated areas like the sky
            blue_mask_raw = blue_mask.copy()
            
            # Create court mask where green overrides blue
            height, width = image.shape[:2]
            court_mask = np.zeros((height, width), dtype=np.uint8)
            court_mask[blue_mask_raw > 0] = 1  # Blue areas
            court_mask[green_mask > 0] = 0     # Green areas override blue
            
            # Filter out blue regions that don't have any green nearby (like sky)
            OutputManager.status("Processing court regions")
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(blue_mask_raw, connectivity=8)
            OutputManager.log(f"Found {num_labels-1} connected blue regions", "INFO")
            
            # For each blue region, check if there's green nearby
            filtered_court_mask = np.zeros_like(court_mask)
            valid_regions = 0
            
            for i in range(1, num_labels):
                region = (labels == i).astype(np.uint8)
                area = stats[i, cv2.CC_STAT_AREA]
                
                # Skip very small regions
                if area < Config.Court.MIN_AREA:
                    continue
                
                # Dilate the region to check for nearby green
                kernel = np.ones((15, 15), np.uint8)
                dilated_region = cv2.dilate(region, kernel, iterations=1)
                
                # Check if there's green nearby this blue region
                green_nearby = cv2.bitwise_and(green_mask, dilated_region)
                green_nearby_pixels = cv2.countNonZero(green_nearby)
                
                # Only keep blue regions that have at least some green nearby
                if green_nearby_pixels > 30:  # Reduced from 50 to be more lenient
                    # This is likely a court (not sky) - keep it
                    filtered_court_mask[region > 0] = court_mask[region > 0]
                    valid_regions += 1
                    if Config.Output.EXTRA_VERBOSE:
                        OutputManager.log(f"Region {i}: area={area}, green nearby={green_nearby_pixels} - likely court", "DEBUG")
            
            OutputManager.log(f"Court regions processed: {valid_regions} valid regions found", "SUCCESS")
            
            # Use the filtered court mask for further processing
            court_mask = filtered_court_mask
        except Exception as e:
            OutputManager.log(f"Error processing court colors: {str(e)}", "ERROR")
            # Continue with blank masks as a fallback
            height, width = image.shape[:2]
            blue_mask_raw = np.zeros((height, width), dtype=np.uint8)
            green_mask = np.zeros((height, width), dtype=np.uint8)
            court_mask = np.zeros((height, width), dtype=np.uint8)
        
        # Save raw masks for debugging
        if debug_folder and Config.DEBUG_MODE:
            try:
                cv2.imwrite(os.path.join(debug_folder, "blue_mask_raw.png"), blue_mask_raw)
                cv2.imwrite(os.path.join(debug_folder, "green_mask.png"), green_mask)
                cv2.imwrite(os.path.join(debug_folder, "filtered_court_mask.png"), court_mask * 255)
                OutputManager.log("Debug masks saved", "DEBUG")
            except Exception as e:
                OutputManager.log(f"Couldn't save debug masks: {str(e)}", "WARNING")
        
        # Create colored visualization of masks
        try:
            court_mask_viz = np.zeros((height, width, 3), dtype=np.uint8)
            court_mask_viz[blue_mask_raw > 0] = [255, 0, 0]  # Blue for all blue areas
            court_mask_viz[green_mask > 0] = [0, 255, 0]     # Green areas override blue
            
            # Highlight filtered courts in a brighter blue
            filtered_blue = np.zeros_like(court_mask_viz)
            filtered_blue[court_mask > 0] = [255, 127, 0]  # Bright blue for valid courts
            cv2.addWeighted(court_mask_viz, 1, filtered_blue, 0.7, 0, court_mask_viz)
        except Exception as e:
            OutputManager.log(f"Error creating court visualization: {str(e)}", "WARNING")
            court_mask_viz = image.copy()  # Use original image as fallback
        
        # Assign court numbers to each separate blue region
        try:
            OutputManager.status("Identifying courts")
            court_numbers_mask, courts = assign_court_numbers(court_mask)
            
            # Output appropriate message based on court detection
            if len(courts) == 0:
                OutputManager.log("No tennis courts found in the image", "WARNING")
            else:
                OutputManager.log(f"Found {len(courts)} tennis court{'s' if len(courts) > 1 else ''}", "SUCCESS")
                if Config.Output.EXTRA_VERBOSE:
                    # Log details of each court
                    for i, court in enumerate(courts):
                        cx, cy = court['centroid']
                        area = court['area']
                        OutputManager.log(f"Court {i+1}: center=({cx}, {cy}), area={area:.1f} pixels", "DEBUG")
        except Exception as e:
            OutputManager.log(f"Error identifying courts: {str(e)}", "ERROR")
            # Create fallback empty data
            courts = []
            court_numbers_mask = np.zeros_like(court_mask)
        
        # Create a color-coded court mask for visualization
        try:
            court_viz = np.zeros((height, width, 3), dtype=np.uint8)
            
            # Assign different colors to each court
            court_colors = [
                (255, 0, 0),    # Blue
                (0, 0, 255),    # Red
                (255, 0, 255),  # Purple
                (0, 255, 255)   # Yellow
            ]
            
            # Draw each court with a different color
            for court in courts:
                court_id = court['court_number']
                color_idx = (court_id - 1) % len(court_colors)
                court_color = court_colors[color_idx]
                
                # Extract court mask
                court_mask_individual = (court_numbers_mask == court_id).astype(np.uint8) * 255
                # Find contours of the court
                court_contours, _ = cv2.findContours(court_mask_individual, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                # Draw the court area
                court_area = np.zeros_like(court_viz)
                court_area[court_mask_individual > 0] = court_color
                cv2.addWeighted(court_viz, 1, court_area, 0.7, 0, court_viz)
                
                # Draw court number at center only if enabled in debug visualizations too
                if hasattr(Config.Visual, 'SHOW_COURT_LABELS') and Config.Visual.SHOW_COURT_LABELS:
                    cx, cy = int(court['centroid'][0]), int(court['centroid'][1])
                    cv2.putText(court_viz, f"Court {court_id}", (cx-40, cy), 
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        except Exception as e:
            OutputManager.log(f"Error creating court visualization: {str(e)}", "WARNING")
            court_viz = image.copy()  # Use original image as fallback
        
        # Save court visualization
        if debug_folder and Config.DEBUG_MODE:
            try:
                cv2.imwrite(os.path.join(debug_folder, "courts_numbered.png"), court_viz)
                OutputManager.log("Court visualization saved", "DEBUG")
            except Exception as e:
                OutputManager.log(f"Couldn't save court visualization: {str(e)}", "WARNING")
        
        # Create a semi-transparent overlay of the masks on the original image
        try:
            alpha = 0.5  # Transparency factor
            mask_overlay = image.copy()
            # Apply the colored masks with transparency
            cv2.addWeighted(court_mask_viz, alpha, mask_overlay, 1 - alpha, 0, mask_overlay)
        except Exception as e:
            OutputManager.log(f"Error creating mask overlay: {str(e)}", "WARNING")
            mask_overlay = image.copy()  # Use original image as fallback
        
        # Detect people
        people = []
        try:
            OutputManager.status("Looking for people")
            
            # Check if models directory exists
            models_dir = Config.Paths.MODELS_DIR
            if not os.path.exists(models_dir):
                try:
                    os.makedirs(models_dir, exist_ok=True)
                    OutputManager.log(f"Created models directory at {models_dir}", "INFO")
                except Exception as e:
                    OutputManager.log(f"Cannot create models directory: {str(e)}", "ERROR")
            
            # Get the model name from config and download if needed
            model_name = Config.Model.NAME
            OutputManager.log(f"Using model: {model_name}", "INFO")
            
            # Check if SSL verification should be disabled
            disable_ssl = False
            if hasattr(args, 'disable_ssl_verify') and args.disable_ssl_verify:
                disable_ssl = True
            
            try:
                # Download model if it doesn't exist (new function)
                model_path = download_yolo_model(
                    model_name, 
                    url=Config.Model.get_model_url(model_name),
                    disable_ssl_verify=disable_ssl
                )
            except Exception as e:
                # If YOLOv8 model fails, try YOLOv5s as fallback
                if model_name != "yolov5s":
                    OutputManager.log(f"Falling back to YOLOv5s model after error with {model_name}", "WARNING")
                    try:
                        model_path = download_yolo_model(
                            "yolov5s", 
                            url=Config.Model.get_model_url("yolov5s"),
                            disable_ssl_verify=True  # Force disable SSL for fallback
                        )
                    except Exception as e2:
                        raise Exception(f"Failed to download fallback model: {str(e2)}")
                else:
                    raise e
            
            # Load the YOLO model with better error handling
            try:
                with suppress_stdout_stderr():
                    OutputManager.status(f"Loading {model_name} model")
                    
                    # Initialize people list and processing flags
                    people = [] 
                    skip_processing = False  # Default to not skip
                    results = None  # Initialize results to None
                    
                    # Determine if this is YOLOv5 or YOLOv8
                    is_yolov8 = model_name.startswith("yolov8")
                    
                    # Check if this is YOLOv12+ which requires ultralytics
                    is_newer_yolo = False
                    for v in range(12, 20):
                        if model_name.lower().startswith(f"yolov{v}"):
                            is_newer_yolo = True
                            break
                    
                    if is_yolov8 or is_newer_yolo:
                        # Check if ultralytics is available
                        if not ULTRALYTICS_AVAILABLE:
                            OutputManager.log("Ultralytics package is not installed. YOLOv8+ requires it.", "ERROR")
                            OutputManager.log("Install with: pip install ultralytics", "INFO")
                            OutputManager.log("Falling back to YOLOv5s model", "WARNING")
                            
                            # Download YOLOv5s instead
                            model_path = download_yolo_model(
                                "yolov5s", 
                                url=Config.Model.get_model_url("yolov5s"),
                                disable_ssl_verify=disable_ssl
                            )
                            model_name = "yolov5s"
                            is_yolov8 = False
                            is_newer_yolo = False
                            
                            # Use YOLOv5 hub for loading
                            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                        else:
                            # Call our test script directly - this is a guaranteed solution
                            try:
                                # Import the test function directly
                                # from test_yolo import test_yolov8_detector  # Using local function defined below
                                OutputManager.log(f"Using direct test function for {model_name}", "INFO")
                                
                                # Force debug mode in Config
                                original_debug = Config.DEBUG_MODE
                                Config.DEBUG_MODE = True
                                
                                # Get people list directly from our tested function - verbose for debugging
                                OutputManager.log("Calling test_yolov8_detector with verbose=True for debugging", "INFO")
                                test_results = test_yolov8_detector(Config.Paths.input_path(), model_path, confidence=0.05, verbose=True)
                                
                                # Restore debug mode
                                Config.DEBUG_MODE = original_debug
                                
                                # Copy results to the people list
                                people = test_results.copy() if test_results else []
                                
                                # Log all detections for debugging
                                for i, person in enumerate(people):
                                    x1, y1, x2, y2 = person['bbox']
                                    conf = person['confidence']
                                    OutputManager.log(f"Person {i+1}: bbox=({x1},{y1},{x2},{y2}), conf={conf:.2f}", "SUCCESS")
                                
                                # Log results
                                OutputManager.log(f"Detected {len(people)} people using direct test function", "SUCCESS")
                                
                                # Skip further processing
                                skip_processing = True
                            except Exception as e:
                                OutputManager.log(f"Error using direct test function: {str(e)}", "ERROR")
                                
                                # Fall back to loading the model directly with ultralytics
                                try:
                                    from ultralytics import YOLO # type: ignore
                                    model = YOLO(model_path)
                                    OutputManager.log(f"Loaded {model_name} directly with YOLO", "SUCCESS")
                                    skip_processing = False
                                except Exception as e2:
                                    OutputManager.log(f"Error loading with YOLO: {str(e2)}", "ERROR")
                                    
                                    # Fall back to standard YOLOv5 approach as last resort
                                    OutputManager.log("Falling back to standard YOLOv5 for model loading", "WARNING")
                                    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                                    skip_processing = False
                    else:
                        # Use YOLOv5 hub for loading
                        model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                    
                    # Set confidence and IoU thresholds
                    if not is_yolov8 and not is_newer_yolo:  # Only apply to YOLOv5 models
                        model.conf = Config.Model.CONFIDENCE
                        model.iou = Config.Model.IOU
                        model.classes = Config.Model.CLASSES
                    
                    OutputManager.log(f"{model_name} model loaded successfully", "SUCCESS")
            except Exception as e:
                # Handle SSL certificate errors
                if "ssl" in str(e).lower():
                    OutputManager.log("SSL certificate error, trying with verification disabled", "WARNING")
                    # Try again with SSL verification disabled
                    ssl._create_default_https_context = ssl._create_unverified_context
                    with suppress_stdout_stderr():
                        model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False, force_reload=True)
                        model.conf = Config.Model.CONFIDENCE
                        model.iou = Config.Model.IOU
                        model.classes = Config.Model.CLASSES
                else:
                    raise e
            
            # Run detection
            OutputManager.status("Running person detection with YOLOv5")
            
            with suppress_stdout_stderr():
                try:
                    # Check if this is a YOLOv8 or newer model
                    if is_yolov8 or is_newer_yolo:
                        # YOLOv8/v12+ models are already handled above
                        # This is just a placeholder to maintain the structure
                        # The actual detection already happened in the model loading section
                        OutputManager.log("YOLOv8+ model detection already completed", "INFO")
                        # Check if we already have people detected from the YOLOv8 model
                        if len(people) > 0:
                            OutputManager.log(f"Using {len(people)} people already detected from YOLOv8", "SUCCESS")
                    else:
                        # Force CPU device on Raspberry Pi Zero (for YOLOv5 models only)
                        model.cpu()
                        OutputManager.log("Using CPU for inference (optimized for Raspberry Pi)", "INFO")
                        
                        # Standard YOLOv5 inference
                        results = model(image)
                        skip_processing = False
                except RuntimeError as e:
                    # Check for memory errors
                    if "out of memory" in str(e).lower():
                        OutputManager.log("Memory error, trying with smaller image", "WARNING")
                        # Try scaling the image down
                        scale_factor = 0.5  # Scale to 50%
                        small_img = cv2.resize(image, (0, 0), fx=scale_factor, fy=scale_factor)
                        
                        if is_yolov8 or is_newer_yolo:
                            # Use our specialized ultralytics detector function for the smaller image
                            people = detect_people_ultralytics(model, small_img, confidence=Config.Model.CONFIDENCE)
                            skip_processing = True
                        else:
                            # Standard YOLOv5 inference on smaller image
                            results = model(small_img)
                            skip_processing = False
                            
                        OutputManager.log(f"Used scaled image ({small_img.shape[1]}x{small_img.shape[0]}) for detection", "INFO")
                    else:
                        raise e
            
            # Process results
            OutputManager.status("Processing detection results")
            
            # Skip processing if YOLOv8/v12 already handled
            if not ('skip_processing' in locals() and skip_processing):
                # Handle different model result formats
                try:
                    # Check if this is YOLOv8 or newer model (ultralytics model)
                    is_newer_yolo = False
                    for v in range(8, 20):  # Support YOLOv8 through YOLOv19
                        if model_name.lower().startswith(f"yolov{v}"):
                            is_newer_yolo = True
                            break
                    
                    # Skip processing if we already extracted people directly
                    if 'skip_processing' in locals() and skip_processing:
                        OutputManager.log("Skipping normal results processing, using direct extraction", "INFO")
                        OutputManager.log(f"People already detected: {len(people)}", "INFO")
                    elif is_newer_yolo:
                        # YOLOv8+ results format (including YOLOv12 and newer)
                        results_data = results
                        
                        # Using YOLOv8+ format (which is a different structure than YOLOv5)
                        if hasattr(results_data, 'boxes'):
                            # Ultralytics API format
                            for box in results_data[0].boxes:  # Access the first result's boxes
                                cls = int(box.cls.item()) if hasattr(box, 'cls') else 0  # Default to person class if not found
                                
                                # Check if this is a person (class 0)
                                if (len(Config.Model.CLASSES) == 0 or cls in Config.Model.CLASSES):
                                    conf = float(box.conf.item()) if hasattr(box, 'conf') else 0.0
                                    
                                    if conf >= Config.Model.CONFIDENCE:
                                        try:
                                            # Handle different box format possibilities
                                            if hasattr(box, 'xyxy'):
                                                if hasattr(box.xyxy, 'cpu'):
                                                    xyxy = box.xyxy.cpu().numpy()[0]
                                                else:
                                                    xyxy = box.xyxy[0].numpy() if hasattr(box.xyxy[0], 'numpy') else box.xyxy[0]
                                            elif hasattr(box, 'xywh'):
                                                # Convert xywh to xyxy
                                                xywh = box.xywh.cpu().numpy()[0] if hasattr(box.xywh, 'cpu') else box.xywh[0]
                                                x, y, w, h = xywh
                                                xyxy = [x-w/2, y-h/2, x+w/2, y+h/2]
                                            else:
                                                # Try a generic approach for other formats
                                                xyxy = box.cpu().numpy()[0] if hasattr(box, 'cpu') else box[0]
                                            
                                            x1, y1, x2, y2 = map(int, xyxy)
                                            
                                            # Calculate center point and foot position
                                            center_x = (x1 + x2) // 2
                                            center_y = (y1 + y2) // 2
                                            foot_x = center_x
                                            foot_y = y2  # Bottom of bounding box represents feet
                                            
                                            # Add to people list
                                            people.append({
                                                'position': (center_x, center_y),
                                                'foot_position': (foot_x, foot_y),
                                                'bbox': (x1, y1, x2, y2),
                                                'confidence': conf
                                            })
                                        except Exception as e:
                                            OutputManager.log(f"Error processing detection: {str(e)}", "ERROR")
                        # Check for different new result formats
                        elif hasattr(results_data, 'pred') and isinstance(results_data.pred, list):
                            # Direct prediction format (used in some newer models)
                            for det in results_data.pred[0]:
                                if len(det) >= 6:  # xyxy, conf, cls
                                    x1, y1, x2, y2 = map(int, det[:4])
                                    conf = float(det[4])
                                    cls_id = int(det[5])
                                    
                                    if (len(Config.Model.CLASSES) == 0 or 
                                        cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                        
                                        # Calculate center point and foot position
                                        center_x = (x1 + x2) // 2
                                        center_y = (y1 + y2) // 2
                                        foot_x = center_x
                                        foot_y = y2  # Bottom of bounding box represents feet
                                        
                                        # Add to people list
                                        people.append({
                                            'position': (center_x, center_y),
                                            'foot_position': (foot_x, foot_y),
                                            'bbox': (x1, y1, x2, y2),
                                            'confidence': conf
                                        })
                        elif isinstance(results_data, list) and len(results_data) > 0:
                            # Alternative format sometimes returned by YOLOv8/YOLOv12
                            detections = None
                            
                            # Handle different tensor/numpy array formats
                            if hasattr(results_data[0], 'numpy'):
                                detections = results_data[0].numpy()
                            elif hasattr(results_data[0], 'cpu'):
                                detections = results_data[0].cpu().numpy()
                            else:
                                detections = results_data[0]
                                
                            if not isinstance(detections, list):
                                # Make sure we iterate over rows
                                if hasattr(detections, 'shape') and len(detections.shape) > 1:
                                    for det in detections:
                                        if len(det) >= 6:  # xyxy, conf, cls
                                            x1, y1, x2, y2 = map(int, det[:4])
                                            conf = float(det[4])
                                            cls_id = int(det[5])
                                            
                                            if (len(Config.Model.CLASSES) == 0 or 
                                                cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                                
                                                # Add to people list
                                                people.append({
                                                    'position': ((x1 + x2) // 2, (y1 + y2) // 2),
                                                    'foot_position': ((x1 + x2) // 2, y2),
                                                    'bbox': (x1, y1, x2, y2),
                                                    'confidence': conf
                                                })
                            else:
                                # Process list-format detections
                                for det in detections:
                                    if len(det) >= 6:  # xyxy, conf, cls
                                        x1, y1, x2, y2 = map(int, det[:4])
                                        conf = float(det[4])
                                        cls_id = int(det[5])
                                        
                                        if (len(Config.Model.CLASSES) == 0 or 
                                            cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                            
                                            # Calculate center point and foot position
                                            center_x = (x1 + x2) // 2
                                            center_y = (y1 + y2) // 2
                                            foot_x = center_x
                                            foot_y = y2  # Bottom of bounding box represents feet
                                            
                                            # Add to people list
                                            people.append({
                                                'position': (center_x, center_y),
                                                'foot_position': (foot_x, foot_y),
                                                'bbox': (x1, y1, x2, y2),
                                                'confidence': conf
                                            })
                            else:
                                # YOLOv5 results format - using pandas
                            df = results.pandas().xyxy[0]
                            df = df[df['class'] == 0]
                            
                            for _, row in df.iterrows():
                                x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
                                
                                # Calculate center point and foot position
                                center_x = (x1 + x2) // 2
                                center_y = (y1 + y2) // 2
                                foot_x = center_x
                                foot_y = y2  # Bottom of bounding box represents feet
                                
                                # Add to people list
                                people.append({
                                    'position': (center_x, center_y),
                                    'foot_position': (foot_x, foot_y),
                                    'bbox': (x1, y1, x2, y2),
                                    'confidence': row['confidence']
                                })
                except Exception as e:
                    error_msg = str(e)
                    
                    # Add more specific error messages for YOLOv8+ issues
                    if "'list' object has no attribute 'pandas'" in error_msg:
                        # Extract the model version
                        version_match = re.search(r'yolov(\d+)', model_name.lower())
                        version = version_match.group(1) if version_match else "newer"
                        OutputManager.log(f"YOLOv{version} result format error. This model requires the ultralytics package.", "ERROR")
                        OutputManager.log("Install ultralytics with: pip install ultralytics", "INFO")
                        OutputManager.log("Or try YOLOv5 instead with: --model yolov5s", "INFO")
                    elif "no attribute 'numpy'" in error_msg or "tensor" in error_msg.lower():
                        OutputManager.log(f"YOLO tensor processing error with {model_name}.", "ERROR")
                        OutputManager.log("Install ultralytics with: pip install ultralytics", "INFO")
                    else:
                        OutputManager.log(f"Problem detecting people: {str(e)}", "ERROR")
                
                # Continue with empty people list
                people = []
            
            # Report how many people we found
            OutputManager.log(f"Found {len(people)} {'person' if len(people) == 1 else 'people'} in the image", "SUCCESS")
            if Config.Output.EXTRA_VERBOSE and people:
                # Log details of detected people
                for i, person in enumerate(people):
                    x1, y1, x2, y2 = person['bbox']
                    conf = person['confidence']
                    OutputManager.log(f"Person {i+1}: bbox=({x1},{y1},{x2},{y2}), confidence={conf:.2f}", "DEBUG")
        except Exception as e:
            error_msg = str(e)
            
            # Handle different types of errors with specific messages
            if "model" in error_msg.lower() or "yolo" in error_msg.lower() or "no such file" in error_msg.lower():
                OutputManager.log(f"YOLOv5 model not found: {error_msg}", "ERROR")
                OutputManager.log("Model missing - run: mkdir -p models && wget -q https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt -O models/yolov5s.pt", "ERROR")
            elif "cuda" in error_msg.lower() or "gpu" in error_msg.lower():
                OutputManager.log(f"CUDA error: {error_msg}", "ERROR")
                OutputManager.log("CUDA error - run: pip install -r requirements.txt", "ERROR")
            elif "ssl" in error_msg.lower():
                OutputManager.log(f"SSL error: {error_msg}", "ERROR")
                OutputManager.log("SSL error - run: pip install certifi --upgrade", "ERROR")
            elif "memory" in error_msg.lower():
                OutputManager.log(f"Memory error: {error_msg}", "ERROR")
                OutputManager.log("Memory error - try using a smaller image or reducing batch size", "ERROR")
            elif "timeout" in error_msg.lower() or "connection" in error_msg.lower():
                OutputManager.log(f"Network error: {error_msg}", "ERROR")
                OutputManager.log("Network error - check your internet connection and try again", "ERROR")
            else:
                OutputManager.log(f"Problem detecting people: {error_msg}", "ERROR")
            
            # Continue with empty people list
            people = []
        
        # Determine if each person is on a court
        people_locations = []
        try:
            if people and courts:
                OutputManager.status("Analyzing positions using multiprocessing")
                
                # Process in parallel using our optimized function
                people_locations = analyze_people_positions_parallel(people, courts)
                
                OutputManager.log("Positions analyzed successfully", "SUCCESS")
                
                # Count people by location type
                in_bounds_count = sum(1 for _, area_type in people_locations if area_type == 'in_bounds')
                out_bounds_count = sum(1 for _, area_type in people_locations if area_type == 'out_bounds')
                off_court_count = sum(1 for _, area_type in people_locations if area_type == 'off_court')
                
                OutputManager.log(f"Position breakdown: {in_bounds_count} in-bounds, {out_bounds_count} sidelines, {off_court_count} off-court", "INFO")
            else:
                # If no people or no courts, no need to analyze positions
                for _ in range(len(people)):
                    people_locations.append((-1, 'off_court'))
        except Exception as e:
            OutputManager.log(f"Error analyzing positions: {str(e)}", "ERROR")
            # Create fallback position data
            for _ in range(len(people)):
                people_locations.append((-1, 'off_court'))
        
        # Calculate court counts for summary
        court_counts = {}
        for court_idx, area_type in people_locations:
            if court_idx >= 0:
                court_num = court_idx + 1
                if court_num not in court_counts:
                    court_counts[court_num] = 0
                court_counts[court_num] += 1
        
        # Log court counts
        for court_num in sorted(court_counts.keys()):
            OutputManager.log(f"Court {court_num}: {court_counts[court_num]} people", "INFO")
        
        # Create debug visualization showing foot positions on mask
        if debug_folder and Config.DEBUG_MODE and people:
            try:
                debug_foot_positions = court_viz.copy()
                for person_idx, person in enumerate(people):
                    if 'foot_position' in person:
                        foot_x, foot_y = person['foot_position']
                        # Draw foot position marker (circle)
                        cv2.circle(debug_foot_positions, (foot_x, foot_y), 10, (255, 255, 255), -1)
                        cv2.circle(debug_foot_positions, (foot_x, foot_y), 10, (0, 0, 0), 2)
                        # Label with person index
                        cv2.putText(debug_foot_positions, f"P{person_idx+1}", (foot_x+15, foot_y), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                cv2.imwrite(os.path.join(debug_folder, "foot_positions_debug.png"), debug_foot_positions)
                OutputManager.log("Foot positions debug image saved", "DEBUG")
            except Exception as e:
                OutputManager.log(f"Couldn't save foot positions debug image: {str(e)}", "WARNING")
        
        # Create final output image
        try:
            OutputManager.status("Creating output image")
            output_image = image.copy()
            
            # Draw court outlines with different colors
            for court in courts:
                court_id = court['court_number']
                color_idx = (court_id - 1) % len(court_colors)
                court_color = court_colors[color_idx]
                
                # Extract court mask
                court_mask_individual = (court_numbers_mask == court_id).astype(np.uint8) * 255
                # Find contours of the court
                court_contours, _ = cv2.findContours(court_mask_individual, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                # Draw the court outline
                cv2.drawContours(output_image, court_contours, -1, court_color, 2)
                
                # Draw court number at center only if enabled
                if Config.Visual.SHOW_COURT_NUMBER:
                    cx, cy = int(court['centroid'][0]), int(court['centroid'][1])
                    cv2.putText(output_image, f"Court {court_id}", (cx-40, cy), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            # Draw people and their locations
            for i, person in enumerate(people):
                court_idx, area_type = people_locations[i]
                
                # Draw bounding box and label
                x1, y1, x2, y2 = person['bbox']
                
                # Choose color based on location
                if court_idx >= 0:
                    court_number = court_idx + 1
                    if area_type == 'in_bounds':
                        color = Config.Visual.PERSON_IN_BOUNDS_COLOR
                        label = f"Court {court_number}" if Config.Visual.SHOW_DETAILED_LABELS else ""
                    else:  # out_bounds
                        color = Config.Visual.PERSON_OUT_BOUNDS_COLOR
                        label = f"Court {court_number} • Sideline" if Config.Visual.SHOW_DETAILED_LABELS else ""
                else:
                    color = Config.Visual.PERSON_OFF_COURT_COLOR
                    label = "Not on court" if Config.Visual.SHOW_DETAILED_LABELS else ""
                
                # Draw bounding box
                cv2.rectangle(output_image, (x1, y1), (x2, y2), color, 2)
                
                # Draw foot position marker - smaller and less intrusive
                foot_x, foot_y = person['foot_position']
                cv2.circle(output_image, (foot_x, foot_y), 3, color, -1)
                
                # Only draw text labels if specified
                if Config.Visual.SHOW_DETAILED_LABELS and label:
                    # Draw label with black background for readability
                    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                               Config.Visual.FONT_SCALE, 
                                               Config.Visual.TEXT_THICKNESS)[0]
                    cv2.rectangle(output_image, (x1, y1 - text_size[1] - 5), 
                                 (x1 + text_size[0], y1), color, -1)
                    cv2.putText(output_image, label, (x1, y1 - 5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 
                               Config.Visual.FONT_SCALE, 
                               Config.Visual.TEXT_COLOR, 
                               Config.Visual.TEXT_THICKNESS)
                    
                    # Add person index number
                    cv2.putText(output_image, f"Person {i+1}", (x1, y2 + 20), 
                                cv2.FONT_HERSHEY_SIMPLEX, Config.Visual.FONT_SCALE, 
                                color, Config.Visual.TEXT_THICKNESS)
                else:
                    # Just add a small number indicator for simpler display
                    cv2.putText(output_image, f"{i+1}", (x1, y1 - 5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 
                               Config.Visual.FONT_SCALE, 
                               Config.Visual.TEXT_COLOR, 
                               Config.Visual.TEXT_THICKNESS)
            
            OutputManager.log("Output image generated", "SUCCESS")
        except Exception as e:
            OutputManager.log(f"Error creating output image: {str(e)}", "ERROR")
            output_image = image.copy()  # Use original image as fallback
        
        # Save the final output image
        output_path = Config.Paths.output_path()
        try:
            OutputManager.status("Saving output image")
            
            # Ensure output directory exists
            output_dir = os.path.dirname(output_path)
            if not os.path.exists(output_dir):
                try:
                    os.makedirs(output_dir, exist_ok=True)
                    OutputManager.log(f"Created output directory at {output_dir}", "INFO")
                except Exception as e:
                    OutputManager.log(f"Cannot create output directory: {str(e)}", "ERROR")
            
            cv2.imwrite(output_path, output_image)
            OutputManager.log(f"Output image saved successfully to {output_path}", "SUCCESS")
        except Exception as e:
            OutputManager.log(f"Error saving output image: {str(e)}", "ERROR")
            output_path = None
        
        # Create the adaptive final summary
        processing_time = time.time() - start_time
        OutputManager.log(f"Total processing time: {processing_time:.2f} seconds", "INFO")
        
        final_summary = OutputManager.create_final_summary(
            people_count=len(people),
            court_counts=court_counts,
            output_path=output_path,
            total_courts=len(courts)  # Pass the total number of courts
        )
        
        # Use the fancy summary method
        OutputManager.fancy_summary(
            "RESULTS SUMMARY", 
            final_summary, 
            processing_time=processing_time
        )
        
        # If there were errors that didn't cause a fatal exit, still indicate an error status
        if OutputManager.errors:
            sys.exit(1)
        
        sys.exit(0)
    except Exception as e:
        # This is the main catch-all for any unhandled exceptions in the try block
        OutputManager.log(f"Unhandled error in main function: {str(e)}", "ERROR")
        
        # Create a basic summary with the error
        processing_time = time.time() - start_time
        final_summary = OutputManager.create_final_summary(
            people_count=None, 
            court_counts={}, 
            output_path=None,
            processing_time=processing_time,
            total_courts=0
        )
        print_error_summary(final_summary)
        sys.exit(1)

# Add a backward compatibility wrapper for the old log function
def log(message, level="INFO"):
    """Wrapper for backward compatibility with the old log function"""
    OutputManager.log(message, level)

def print_error_summary(final_summary):
    """Print summary using fancy_summary, handling errors and warnings"""
    # First clear any lingering output
    sys.stdout.write("\r\033[K")
    sys.stdout.flush()
    
    # Use the fancy summary with appropriate title
    if OutputManager.errors:
        OutputManager.fancy_summary("ERROR SUMMARY", final_summary)
    elif OutputManager.warnings:
        OutputManager.fancy_summary("WARNING SUMMARY", final_summary)
    else:
        # If no errors or warnings, but called, print a general summary
        OutputManager.fancy_summary("PROCESS SUMMARY", final_summary)
    
    # Flush to ensure immediate display
    sys.stdout.flush()

def download_yolo_model(model_name, url=None, disable_ssl_verify=False):
    """Download YOLO model if it doesn't exist"""
    # Create models directory if it doesn't exist
    os.makedirs(Config.Paths.MODELS_DIR, exist_ok=True)
    
    model_path = os.path.join(Config.Paths.MODELS_DIR, f"{model_name}.pt")
    
    # Check if model exists
    if os.path.exists(model_path):
        OutputManager.log(f"Model {model_name} found at {model_path}", "SUCCESS")
        return model_path
    
    # Get URL if not provided
    if url is None:
        url = Config.Model.get_model_url(model_name)
    
    OutputManager.log(f"Model {model_name} not found at {model_path}", "WARNING")
    OutputManager.status(f"Downloading {model_name} from {url}")
    
    try:
        # Handle SSL verification
        if disable_ssl_verify:
            # Disable SSL verification entirely
            ssl._create_default_https_context = ssl._create_unverified_context
            OutputManager.log("SSL verification disabled for download", "WARNING")
        elif sys.platform == 'darwin':  # macOS specific fix
            # On macOS, install certificates if needed
            try:
                import certifi
                ssl_context = ssl.create_default_context(cafile=certifi.where())
            except ImportError:
                # If certifi is not installed, try to use macOS certificates
                OutputManager.log("Installing SSL certificates for macOS...", "INFO")
                import subprocess
                subprocess.call(['/Applications/Python 3.x/Install Certificates.command'], shell=True)
                # If that fails, we'll continue without it and might encounter errors
        
        # Create a progress reporter for the download
        def report_progress(blocknum, blocksize, totalsize):
            if totalsize > 0:
                percent = min(blocknum * blocksize * 100 / totalsize, 100)
                OutputManager.status(f"Downloading {model_name}: {percent:.1f}% of {totalsize/1024/1024:.1f} MB")
        
        # Download the file
        try:
            urllib.request.urlretrieve(url, model_path, report_progress)
            OutputManager.log(f"Model {model_name} downloaded successfully", "SUCCESS")
            return model_path
        except (urllib.error.URLError, ssl.SSLError) as e:
            OutputManager.log(f"Error downloading model: {str(e)}", "ERROR")
            
            # Special handling for SSL certificate errors
            if isinstance(e, ssl.SSLCertVerificationError) or "CERTIFICATE_VERIFY_FAILED" in str(e):
                OutputManager.log("SSL certificate verification failed. Try running with --disable-ssl-verify", "WARNING")
                OutputManager.log("Alternatively, you can manually download the model:", "INFO")
                OutputManager.log(f"1. Download from: {url}", "INFO")
                OutputManager.log(f"2. Save it to: {model_path}", "INFO")
                
                # For macOS users, provide specific instructions
                if sys.platform == 'darwin':
                    OutputManager.log("For macOS users, run the following command in Terminal:", "INFO")
                    OutputManager.log("/Applications/Python 3.x/Install Certificates.command", "INFO")
                    OutputManager.log("Replace '3.x' with your Python version (e.g., 3.9, 3.10)", "INFO")
            
            # Try an alternative URL if this is a YOLOv8 model
            if model_name.startswith("yolov8") and "ultralytics/assets" in url:
                alt_url = f"https://github.com/ultralytics/ultralytics/releases/download/v8.0.0/{model_name}.pt"
                OutputManager.log(f"Trying alternative URL for {model_name}: {alt_url}", "INFO")
                try:
                    urllib.request.urlretrieve(alt_url, model_path, report_progress)
                    OutputManager.log(f"Model {model_name} downloaded successfully from alternative URL", "SUCCESS")
                    return model_path
                except Exception as e2:
                    OutputManager.log(f"Alternative download also failed: {str(e2)}", "ERROR")
            
            raise Exception(f"Model file not found and could not be downloaded. Error: {str(e)}")
    
    except Exception as e:
        # Clean up any partially downloaded file
        if os.path.exists(model_path):
            os.remove(model_path)
        
        # Log the error and re-raise
        OutputManager.log(f"Failed to download {model_name}: {str(e)}", "ERROR")
        raise

# Function to install ultralytics package
def install_ultralytics():
    """Install ultralytics package for YOLOv8 models"""
    print("======================================================")
    print("  Installing ultralytics package for YOLOv8 models")
    print("======================================================")
    
    try:
        # Check if pip is available
        subprocess.check_call([sys.executable, "-m", "pip", "--version"])
        
        # Install ultralytics
        print("\nInstalling ultralytics package...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "ultralytics"])
        
        # Verify installation
        print("\nVerifying installation...")
        subprocess.check_call([sys.executable, "-c", "import ultralytics; print(f'Ultralytics version: {ultralytics.__version__}')"])
        
        print("\n✅ Installation successful!")
        print("\nYou can now use YOLOv8 models with the main script.")
        print("Try running: python main.py --model yolov8x")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"\n❌ Error: {str(e)}")
        print("\nManual installation instructions:")
        print("1. Open a terminal or command prompt")
        print("2. Run: pip install ultralytics")
        print("3. Then run the main script with: python main.py --model yolov8x")
        return False
    except Exception as e:
        print(f"\n❌ Unexpected error: {str(e)}")
        return False

# Function to test YOLOv8 detection
def test_yolov8_detector(image_path, model_name="yolov8x.pt", confidence=0.15, verbose=True):
    """Test YOLOv8 detection on an image"""
    if verbose:
        print(f"Testing YOLOv8 detection on {image_path} with model {model_name}")
    
    # Check if ultralytics is available
    try:
        from ultralytics import YOLO
    except ImportError:
        if verbose:
            print("ERROR: ultralytics package is not installed.")
            print("Install with: pip install ultralytics or use the install_ultralytics() function")
        return []
    
    # Load the model
    try:
        model = YOLO(model_name)
    except Exception as e:
        if verbose:
            print(f"Error loading model: {str(e)}")
        return []
    
    # Load the image
    if os.path.exists(image_path):
        if verbose:
            print(f"Image found: {image_path}")
        image = cv2.imread(image_path)
        if verbose:
            print(f"Image size: {image.shape}")
    else:
        if verbose:
            print(f"Image not found: {image_path}")
        return []
    
    # Run prediction
    try:
        results = model.predict(
            source=image,
            conf=confidence,  # Person class only
            classes=[0],      # Person class only
            verbose=verbose,  # Only show output if verbose
            save=False,       # Don't save output images
            project="test_output",
            name="yolo_test"
        )
    except Exception as e:
        if verbose:
            print(f"Error during prediction: {str(e)}")
        return []
    
    # Process results
    people = []
    if len(results) > 0:
        if verbose:
            print(f"\nResults type: {type(results)}")
            print(f"Number of results: {len(results)}")
        
        # Check for boxes
        if hasattr(results[0], 'boxes'):
            boxes = results[0].boxes
            if verbose:
                print(f"\nDetected boxes: {len(boxes)}")
            
            # Process each detection
            for i, box in enumerate(boxes):
                try:
                    cls = int(box.cls.item()) if hasattr(box, 'cls') else -1
                    conf = float(box.conf.item()) if hasattr(box, 'conf') else 0
                    
                    if cls == 0:  # Person class
                        # Get coordinates
                        xyxy = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = map(int, xyxy)
                        
                        # Add to people list
                        person = {
                            'position': ((x1 + x2) // 2, (y1 + y2) // 2),
                            'foot_position': ((x1 + x2) // 2, y2),
                            'bbox': (x1, y1, x2, y2),
                            'confidence': conf
                        }
                        people.append(person)
                        if verbose:
                            print(f"Person {i+1}: bbox=({x1},{y1},{x2},{y2}), conf={conf:.2f}")
                except Exception as e:
                    if verbose:
                        print(f"Error processing detection {i}: {str(e)}")
            
            if verbose:
                print(f"\nTotal people detected: {len(people)}")
        elif verbose:
            print("No boxes attribute found in results")
    elif verbose:
        print("No results returned from model")
    
    # Return the people list for use in main.py
    return people

def detect_people(image_path, model_path="models/yolov8n.pt"):
    """
    Detect people in an image using the YOLO model.
    Returns list of detected people and the loaded image.
    """
    try:
        # Load the image
        image = cv2.imread(image_path)
        if image is None:
            OutputManager.log(f"Failed to load image from {image_path}", "ERROR")
            return [], None
        
        # Check if we can use ultralytics for YOLOv8+ models
        is_v8_or_newer = False
        for v in range(8, 20):
            if f"yolov{v}" in model_path.lower():
                is_v8_or_newer = True
                break
                
        if is_v8_or_newer and ULTRALYTICS_AVAILABLE:
            try:
                # Use ultralytics API for YOLOv8+ models
                from ultralytics import YOLO
                model = YOLO(model_path)
                
                # Use the specialized function for ultralytics models
                return detect_people_ultralytics(model, image, confidence=Config.Model.CONFIDENCE), image
            except Exception as e:
                OutputManager.log(f"Error using ultralytics API: {str(e)}", "ERROR")
                # Fall back to default processing
        
        # Load YOLO model using torch hub
        try:
            with suppress_stdout_stderr():
                model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                model.conf = Config.Model.CONFIDENCE
                model.classes = [0]  # Only detect people
        except Exception as e:
            OutputManager.log(f"Error loading YOLO model: {str(e)}", "ERROR")
            return [], image
        
        # Run detection
        try:
            with suppress_stdout_stderr():
                results = model(image)
            
            # Process results
            people = []
            try:
                # Extract detections from pandas dataframe
                df = results.pandas().xyxy[0]
                df = df[df['class'] == 0]  # Only keep people (class 0)
                
                for _, row in df.iterrows():
                    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
                    confidence = float(row['confidence'])
                    
                    # Calculate center point and foot position
                    center_x = (x1 + x2) // 2
                    center_y = (y1 + y2) // 2
                    foot_x = center_x
                    foot_y = y2  # Bottom of bounding box represents feet
                    
                    # Add to people list
                    people.append({
                        'position': (center_x, center_y),
                        'foot_position': (foot_x, foot_y),
                        'bbox': (x1, y1, x2, y2),
                        'confidence': confidence
                    })
                
                OutputManager.log(f"Found {len(people)} people in the image", "SUCCESS")
                return people, image
                
            except Exception as e:
                OutputManager.log(f"Error processing detection results: {str(e)}", "ERROR")
                return [], image
                
        except Exception as e:
            OutputManager.log(f"Error running detection: {str(e)}", "ERROR")
            return [], image
            
    except Exception as e:
        OutputManager.log(f"Error in person detection: {str(e)}", "ERROR")
        return [], None

if __name__ == "__main__":
    try:
        # Add command-line arguments for easier use
        parser = argparse.ArgumentParser(description="Tennis Court Detection System")
        parser.add_argument("--input", type=str, help="Path to input image", default=Config.Paths.input_path())
        parser.add_argument("--output", type=str, help="Path for output image", default=Config.Paths.output_path())
        parser.add_argument("--debug", action="store_true", help="Enable debug mode with additional outputs")
        parser.add_argument("--quiet", action="store_true", help="Reduce console output")
        parser.add_argument("--show-labels", action="store_true", help="Show detailed labels on output image")
        parser.add_argument("--show-court-labels", action="store_true", help="Show court numbers on output image")
        parser.add_argument("--device", type=str, choices=["cpu", "cuda"], help="Device to use for inference", default=None)
        parser.add_argument("--disable-ssl-verify", action="store_true", help="Disable SSL verification for downloads")
        parser.add_argument("--model", type=str, help="YOLO model to use (yolov5s, yolov5m, yolov5l, etc.)", default=Config.Model.NAME)
        parser.add_argument("--no-multiprocessing", action="store_true", help="Disable multiprocessing")
        parser.add_argument("--processes", type=int, help="Number of processes to use for multiprocessing", default=Config.MultiProcessing.NUM_PROCESSES)
        parser.add_argument("--extra-verbose", action="store_true", help="Show extra detailed output for debugging")
        parser.add_argument("--force-macos-cert-install", action="store_true", help="Force macOS certificate installation")
        # Add new arguments for merged functionality
        parser.add_argument("--install-ultralytics", action="store_true", help="Install the ultralytics package")
        parser.add_argument("--test-yolo", action="store_true", help="Run YOLO model test on the input image")
        parser.add_argument("--no-camera", action="store_true", help="Disable camera usage (useful for testing without hardware)")
        parser.add_argument("--camera-resolution", type=str, help="Set camera resolution as width,height (e.g., 640,480)", default=None)
        
        # Parse arguments
        try:
            args = parser.parse_args()
        except Exception as e:
            print(f"\nError parsing command-line arguments: {str(e)}")
            print("Run with --help for usage information")
            sys.exit(1)
            
        # Handle new merged functionality
        if args.install_ultralytics:
            success = install_ultralytics()
            sys.exit(0 if success else 1)
            
        if args.test_yolo:
            # Get image path from command line
            image_path = args.input
            
            # Ensure models directory exists
            if not os.path.exists(Config.Paths.MODELS_DIR):
                os.makedirs(Config.Paths.MODELS_DIR, exist_ok=True)
                print(f"Created models directory at {Config.Paths.MODELS_DIR}")
            
            # Run the test with verbose output
            model_name = args.model
            test_yolov8_detector(image_path, model_name=model_name, verbose=not args.quiet)
            sys.exit(0)
        
        # Handle --no-camera flag
        if args.no_camera:
            if Config.Output.VERBOSE:
                print("Camera explicitly disabled via --no-camera flag.")
            CAMERA_AVAILABLE = False
            # Redefine takePhoto as dummy if camera disabled late
            def takePhoto(resolution=DEFAULT_CAMERA_RESOLUTION, output_file='images/input.png'):
                if Config.Output.VERBOSE:
                    print(f"Camera disabled. Skipping photo capture to {output_file}.")
                return False

        OutputManager.reset_logs()

        # Record start time for performance measurement
        start_time = time.time()
        
        # Create necessary directories
        os.makedirs(Config.Paths.IMAGES_DIR, exist_ok=True)
        os.makedirs(Config.Paths.MODELS_DIR, exist_ok=True)
        os.makedirs(Config.Paths.OUTPUT_DIR, exist_ok=True)
        
        try:
            # Take photo only if camera is available
            if CAMERA_AVAILABLE and not args.input_image:
                # Check if camera resolution is specified
                camera_resolution = (1920, 1080)  # Default resolution
                if hasattr(args, 'camera_resolution') and args.camera_resolution:
                    try:
                        width, height = args.camera_resolution.split(',')
                        camera_resolution = (int(width), int(height))
                        OutputManager.log(f"Using custom camera resolution: {camera_resolution}", "INFO")
                    except (ValueError, IndexError):
                        OutputManager.log(f"Invalid camera resolution format: {args.camera_resolution}. Using default.", "WARNING")
                    
                success = takePhoto(resolution=camera_resolution) # take a photo through the camera
                if success:
                    OutputManager.log("Photo captured successfully.", "SUCCESS")
            elif args.input_image:
                # Copy the input image to the standard location
                OutputManager.log(f"Using provided input image: {args.input_image}", "INFO")
                try:
                    shutil.copy(args.input_image, Config.Paths.input_path())
                    OutputManager.log(f"Copied input image to {Config.Paths.input_path()}", "SUCCESS")
                except Exception as copy_error:
                    OutputManager.log(f"Error copying input image: {copy_error}", "ERROR")
            else:
                # Create a blank image if camera not available and no input
                if not os.path.exists(Config.Paths.input_path()):
                    # Create a placeholder black image
                    if Config.Output.VERBOSE:
                        print(f"Creating a placeholder black image at {Config.Paths.input_path()}")
                    
                    # Create the blank image with the specified resolution
                    dummy_image = np.zeros((DEFAULT_CAMERA_RESOLUTION[1], DEFAULT_CAMERA_RESOLUTION[0], 3), dtype=np.uint8)
                    # Ensure directory exists
                    os.makedirs(os.path.dirname(Config.Paths.input_path()), exist_ok=True)
                    # Save the image
                    cv2.imwrite(Config.Paths.input_path(), dummy_image)
                    if Config.Output.VERBOSE:
                        print(f"Created placeholder image at {Config.Paths.input_path()}")
                    
                OutputManager.log("Skipping photo capture as camera is not available.", "WARNING")
        except Exception as e:
            OutputManager.log(f"Error capturing photo: {e}", "ERROR")
            sys.exit(1)
        
        # Initialize multiprocessing if enabled
        if Config.MultiProcessing.ENABLED:
            # Set the number of processes if not already set
            if Config.MultiProcessing.NUM_PROCESSES <= 0:
                # Use 3 processes or the number of CPU cores if less than 3
                Config.MultiProcessing.NUM_PROCESSES = min(3, cpu_count())
            
            OutputManager.log(f"Multiprocessing enabled with {Config.MultiProcessing.NUM_PROCESSES} processes", "INFO")
        
        try:
            # Load image
            input_path = Config.Paths.input_path()
            try:
                # First ensure the images directory exists
                images_dir = os.path.dirname(input_path)
                if not os.path.exists(images_dir):
                    try:
                        os.makedirs(images_dir, exist_ok=True)
                        OutputManager.log(f"Created images directory at {images_dir}", "INFO")
                    except Exception as e:
                        OutputManager.log(f"Cannot create images directory: {str(e)}", "ERROR")
                
                # Load the image
                OutputManager.status("Loading image")
                image = cv2.imread(input_path)
                
                # Check image loaded successfully
                if image is not None:
                    OutputManager.log(f"Image loaded successfully: {image.shape[1]}x{image.shape[0]} pixels", "SUCCESS")
                    if Config.Output.EXTRA_VERBOSE:
                        OutputManager.log(f"Image type: {image.dtype}, channels: {image.shape[2]}", "INFO")
                else:
                    OutputManager.log(f"Unable to open the image at {input_path}", "ERROR")
                    # Show final summary with error and exit
                    processing_time = time.time() - start_time
                    final_summary = OutputManager.create_final_summary(
                        people_count=None, 
                        court_counts={}, 
                        output_path=None,
                        processing_time=processing_time,
                        total_courts=0
                    )
                    print_error_summary(final_summary)
                    sys.exit(1)
            except Exception as e:
                OutputManager.log(f"Problem loading the image: {str(e)}", "ERROR")
                processing_time = time.time() - start_time
                final_summary = OutputManager.create_final_summary(
                    people_count=None, 
                    court_counts={}, 
                    output_path=None,
                    processing_time=processing_time,
                    total_courts=0
                )
                print_error_summary(final_summary)
                sys.exit(1)
            
            # Set up debug folder
            try:
                if Config.DEBUG_MODE:
                    debug_folder = Config.Paths.debug_dir()
                    os.makedirs(debug_folder, exist_ok=True)
                    OutputManager.log(f"Debug folder created at {debug_folder}", "DEBUG")
                else:
                    debug_folder = None
            except Exception as e:
                OutputManager.log(f"Can't create debug folder: {str(e)}", "WARNING")
                debug_folder = None  # Set to None to prevent further debug saves
                # Continue execution even if debug folder can't be created
            
            # Detect tennis courts
            try:
                OutputManager.status("Analyzing court colors")
                blue_mask = create_blue_mask(image)
                green_mask = create_green_mask(image)
                OutputManager.log("Court colors analyzed", "SUCCESS")
                if Config.Output.EXTRA_VERBOSE:
                    OutputManager.log(f"Blue mask: {np.count_nonzero(blue_mask)} pixels, Green mask: {np.count_nonzero(green_mask)} pixels", "INFO")
                
                # Process the raw blue mask to avoid connecting unrelated areas like the sky
                blue_mask_raw = blue_mask.copy()
                
                # Create court mask where green overrides blue
                height, width = image.shape[:2]
                court_mask = np.zeros((height, width), dtype=np.uint8)
                court_mask[blue_mask_raw > 0] = 1  # Blue areas
                court_mask[green_mask > 0] = 0     # Green areas override blue
                
                # Filter out blue regions that don't have any green nearby (like sky)
                OutputManager.status("Processing court regions")
                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(blue_mask_raw, connectivity=8)
                OutputManager.log(f"Found {num_labels-1} connected blue regions", "INFO")
                
                # For each blue region, check if there's green nearby
                filtered_court_mask = np.zeros_like(court_mask)
                valid_regions = 0
                
                for i in range(1, num_labels):
                    region = (labels == i).astype(np.uint8)
                    area = stats[i, cv2.CC_STAT_AREA]
                    
                    # Skip very small regions
                    if area < Config.Court.MIN_AREA:
                        continue
                    
                    # Dilate the region to check for nearby green
                    kernel = np.ones((15, 15), np.uint8)
                    dilated_region = cv2.dilate(region, kernel, iterations=1)
                    
                    # Check if there's green nearby this blue region
                    green_nearby = cv2.bitwise_and(green_mask, dilated_region)
                    green_nearby_pixels = cv2.countNonZero(green_nearby)
                    
                    # Only keep blue regions that have at least some green nearby
                    if green_nearby_pixels > 30:  # Reduced from 50 to be more lenient
                        # This is likely a court (not sky) - keep it
                        filtered_court_mask[region > 0] = court_mask[region > 0]
                        valid_regions += 1
                        if Config.Output.EXTRA_VERBOSE:
                            OutputManager.log(f"Region {i}: area={area}, green nearby={green_nearby_pixels} - likely court", "DEBUG")
                
                OutputManager.log(f"Court regions processed: {valid_regions} valid regions found", "SUCCESS")
                
                # Use the filtered court mask for further processing
                court_mask = filtered_court_mask
            except Exception as e:
                OutputManager.log(f"Error processing court colors: {str(e)}", "ERROR")
                # Continue with blank masks as a fallback
                height, width = image.shape[:2]
                blue_mask_raw = np.zeros((height, width), dtype=np.uint8)
                green_mask = np.zeros((height, width), dtype=np.uint8)
                court_mask = np.zeros((height, width), dtype=np.uint8)
            
            # Save raw masks for debugging
            if debug_folder and Config.DEBUG_MODE:
                try:
                    cv2.imwrite(os.path.join(debug_folder, "blue_mask_raw.png"), blue_mask_raw)
                    cv2.imwrite(os.path.join(debug_folder, "green_mask.png"), green_mask)
                    cv2.imwrite(os.path.join(debug_folder, "filtered_court_mask.png"), court_mask * 255)
                    OutputManager.log("Debug masks saved", "DEBUG")
                except Exception as e:
                    OutputManager.log(f"Couldn't save debug masks: {str(e)}", "WARNING")
            
            # Create colored visualization of masks
            try:
                court_mask_viz = np.zeros((height, width, 3), dtype=np.uint8)
                court_mask_viz[blue_mask_raw > 0] = [255, 0, 0]  # Blue for all blue areas
                court_mask_viz[green_mask > 0] = [0, 255, 0]     # Green areas override blue
                
                # Highlight filtered courts in a brighter blue
                filtered_blue = np.zeros_like(court_mask_viz)
                filtered_blue[court_mask > 0] = [255, 127, 0]  # Bright blue for valid courts
                cv2.addWeighted(court_mask_viz, 1, filtered_blue, 0.7, 0, court_mask_viz)
            except Exception as e:
                OutputManager.log(f"Error creating court visualization: {str(e)}", "WARNING")
                court_mask_viz = image.copy()  # Use original image as fallback
            
            # Assign court numbers to each separate blue region
            try:
                OutputManager.status("Identifying courts")
                court_numbers_mask, courts = assign_court_numbers(court_mask)
                
                # Output appropriate message based on court detection
                if len(courts) == 0:
                    OutputManager.log("No tennis courts found in the image", "WARNING")
                else:
                    OutputManager.log(f"Found {len(courts)} tennis court{'s' if len(courts) > 1 else ''}", "SUCCESS")
                    if Config.Output.EXTRA_VERBOSE:
                        # Log details of each court
                        for i, court in enumerate(courts):
                            cx, cy = court['centroid']
                            area = court['area']
                            OutputManager.log(f"Court {i+1}: center=({cx}, {cy}), area={area:.1f} pixels", "DEBUG")
            except Exception as e:
                OutputManager.log(f"Error identifying courts: {str(e)}", "ERROR")
                # Create fallback empty data
                courts = []
                court_numbers_mask = np.zeros_like(court_mask)
            
            # Create a color-coded court mask for visualization
            try:
                court_viz = np.zeros((height, width, 3), dtype=np.uint8)
                
                # Assign different colors to each court
                court_colors = [
                    (255, 0, 0),    # Blue
                    (0, 0, 255),    # Red
                    (255, 0, 255),  # Purple
                    (0, 255, 255)   # Yellow
                ]
                
                # Draw each court with a different color
                for court in courts:
                    court_id = court['court_number']
                    color_idx = (court_id - 1) % len(court_colors)
                    court_color = court_colors[color_idx]
                    
                    # Extract court mask
                    court_mask_individual = (court_numbers_mask == court_id).astype(np.uint8) * 255
                    # Find contours of the court
                    court_contours, _ = cv2.findContours(court_mask_individual, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    
                    # Draw the court area
                    court_area = np.zeros_like(court_viz)
                    court_area[court_mask_individual > 0] = court_color
                    cv2.addWeighted(court_viz, 1, court_area, 0.7, 0, court_viz)
                    
                    # Draw court number at center only if enabled in debug visualizations too
                    if hasattr(Config.Visual, 'SHOW_COURT_LABELS') and Config.Visual.SHOW_COURT_LABELS:
                        cx, cy = int(court['centroid'][0]), int(court['centroid'][1])
                        cv2.putText(court_viz, f"Court {court_id}", (cx-40, cy), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            except Exception as e:
                OutputManager.log(f"Error creating court visualization: {str(e)}", "WARNING")
                court_viz = image.copy()  # Use original image as fallback
            
            # Save court visualization
            if debug_folder and Config.DEBUG_MODE:
                try:
                    cv2.imwrite(os.path.join(debug_folder, "courts_numbered.png"), court_viz)
                    OutputManager.log("Court visualization saved", "DEBUG")
                except Exception as e:
                    OutputManager.log(f"Couldn't save court visualization: {str(e)}", "WARNING")
            
            # Create a semi-transparent overlay of the masks on the original image
            try:
                alpha = 0.5  # Transparency factor
                mask_overlay = image.copy()
                # Apply the colored masks with transparency
                cv2.addWeighted(court_mask_viz, alpha, mask_overlay, 1 - alpha, 0, mask_overlay)
            except Exception as e:
                OutputManager.log(f"Error creating mask overlay: {str(e)}", "WARNING")
                mask_overlay = image.copy()  # Use original image as fallback
            
            # Detect people
            people = []
            try:
                OutputManager.status("Looking for people")
                
                # Check if models directory exists
                models_dir = Config.Paths.MODELS_DIR
                if not os.path.exists(models_dir):
                    try:
                        os.makedirs(models_dir, exist_ok=True)
                        OutputManager.log(f"Created models directory at {models_dir}", "INFO")
                    except Exception as e:
                        OutputManager.log(f"Cannot create models directory: {str(e)}", "ERROR")
                
                # Get the model name from config and download if needed
                model_name = Config.Model.NAME
                OutputManager.log(f"Using model: {model_name}", "INFO")
                
                # Check if SSL verification should be disabled
                disable_ssl = False
                if hasattr(args, 'disable_ssl_verify') and args.disable_ssl_verify:
                    disable_ssl = True
                
                try:
                    # Download model if it doesn't exist (new function)
                    model_path = download_yolo_model(
                        model_name, 
                        url=Config.Model.get_model_url(model_name),
                        disable_ssl_verify=disable_ssl
                    )
                except Exception as e:
                    # If YOLOv8 model fails, try YOLOv5s as fallback
                    if model_name != "yolov5s":
                        OutputManager.log(f"Falling back to YOLOv5s model after error with {model_name}", "WARNING")
                        try:
                            model_path = download_yolo_model(
                                "yolov5s", 
                                url=Config.Model.get_model_url("yolov5s"),
                                disable_ssl_verify=True  # Force disable SSL for fallback
                            )
                        except Exception as e2:
                            raise Exception(f"Failed to download fallback model: {str(e2)}")
                    else:
                        raise e
                
                # Load the YOLO model with better error handling
                try:
                    with suppress_stdout_stderr():
                        OutputManager.status(f"Loading {model_name} model")
                        
                        # Initialize people list and processing flags
                        people = [] 
                        skip_processing = False  # Default to not skip
                        results = None  # Initialize results to None
                        
                        # Determine if this is YOLOv5 or YOLOv8
                        is_yolov8 = model_name.startswith("yolov8")
                        
                        # Check if this is YOLOv12+ which requires ultralytics
                        is_newer_yolo = False
                        for v in range(12, 20):
                            if model_name.lower().startswith(f"yolov{v}"):
                                is_newer_yolo = True
                                break
                        
                        if is_yolov8 or is_newer_yolo:
                            # Check if ultralytics is available
                            if not ULTRALYTICS_AVAILABLE:
                                OutputManager.log("Ultralytics package is not installed. YOLOv8+ requires it.", "ERROR")
                                OutputManager.log("Install with: pip install ultralytics", "INFO")
                                OutputManager.log("Falling back to YOLOv5s model", "WARNING")
                                
                                # Download YOLOv5s instead
                                model_path = download_yolo_model(
                                    "yolov5s", 
                                    url=Config.Model.get_model_url("yolov5s"),
                                    disable_ssl_verify=disable_ssl
                                )
                                model_name = "yolov5s"
                                is_yolov8 = False
                                is_newer_yolo = False
                                
                                # Use YOLOv5 hub for loading
                                model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                            else:
                                # Call our test script directly - this is a guaranteed solution
                                try:
                                    # Import the test function directly
                                    # from test_yolo import test_yolov8_detector  # Using local function defined below
                                    OutputManager.log(f"Using direct test function for {model_name}", "INFO")
                                    
                                    # Force debug mode in Config
                                    original_debug = Config.DEBUG_MODE
                                    Config.DEBUG_MODE = True
                                    
                                    # Get people list directly from our tested function - verbose for debugging
                                    OutputManager.log("Calling test_yolov8_detector with verbose=True for debugging", "INFO")
                                    test_results = test_yolov8_detector(Config.Paths.input_path(), model_path, confidence=0.05, verbose=True)
                                    
                                    # Restore debug mode
                                    Config.DEBUG_MODE = original_debug
                                    
                                    # Copy results to the people list
                                    people = test_results.copy() if test_results else []
                                    
                                    # Log all detections for debugging
                                    for i, person in enumerate(people):
                                        x1, y1, x2, y2 = person['bbox']
                                        conf = person['confidence']
                                        OutputManager.log(f"Person {i+1}: bbox=({x1},{y1},{x2},{y2}), conf={conf:.2f}", "SUCCESS")
                                    
                                    # Log results
                                    OutputManager.log(f"Detected {len(people)} people using direct test function", "SUCCESS")
                                    
                                    # Skip further processing
                                    skip_processing = True
                                except Exception as e:
                                    OutputManager.log(f"Error using direct test function: {str(e)}", "ERROR")
                                    
                                    # Fall back to loading the model directly with ultralytics
                                    try:
                                        from ultralytics import YOLO # type: ignore
                                        model = YOLO(model_path)
                                        OutputManager.log(f"Loaded {model_name} directly with YOLO", "SUCCESS")
                                        skip_processing = False
                                    except Exception as e2:
                                        OutputManager.log(f"Error loading with YOLO: {str(e2)}", "ERROR")
                                        
                                        # Fall back to standard YOLOv5 approach as last resort
                                        OutputManager.log("Falling back to standard YOLOv5 for model loading", "WARNING")
                                        model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                                        skip_processing = False
                        else:
                            # Use YOLOv5 hub for loading
                            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False)
                        
                        # Set confidence and IoU thresholds
                        if not is_yolov8 and not is_newer_yolo:  # Only apply to YOLOv5 models
                            model.conf = Config.Model.CONFIDENCE
                            model.iou = Config.Model.IOU
                            model.classes = Config.Model.CLASSES
                        
                        OutputManager.log(f"{model_name} model loaded successfully", "SUCCESS")
                except Exception as e:
                    # Handle SSL certificate errors
                    if "ssl" in str(e).lower():
                        OutputManager.log("SSL certificate error, trying with verification disabled", "WARNING")
                        # Try again with SSL verification disabled
                        ssl._create_default_https_context = ssl._create_unverified_context
                        with suppress_stdout_stderr():
                            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, verbose=False, force_reload=True)
                            model.conf = Config.Model.CONFIDENCE
                            model.iou = Config.Model.IOU
                            model.classes = Config.Model.CLASSES
                    else:
                        raise e
                
                # Run detection
                OutputManager.status("Running person detection with YOLOv5")
                
                with suppress_stdout_stderr():
                    try:
                        # Check if this is a YOLOv8 or newer model
                        if is_yolov8 or is_newer_yolo:
                            # YOLOv8/v12+ models are already handled above
                            # This is just a placeholder to maintain the structure
                            # The actual detection already happened in the model loading section
                            OutputManager.log("YOLOv8+ model detection already completed", "INFO")
                            # Check if we already have people detected from the YOLOv8 model
                            if len(people) > 0:
                                OutputManager.log(f"Using {len(people)} people already detected from YOLOv8", "SUCCESS")
                        else:
                            # Force CPU device on Raspberry Pi Zero (for YOLOv5 models only)
                            model.cpu()
                            OutputManager.log("Using CPU for inference (optimized for Raspberry Pi)", "INFO")
                            
                            # Standard YOLOv5 inference
                            results = model(image)
                            skip_processing = False
                    except RuntimeError as e:
                        # Check for memory errors
                        if "out of memory" in str(e).lower():
                            OutputManager.log("Memory error, trying with smaller image", "WARNING")
                            # Try scaling the image down
                            scale_factor = 0.5  # Scale to 50%
                            small_img = cv2.resize(image, (0, 0), fx=scale_factor, fy=scale_factor)
                            
                            if is_yolov8 or is_newer_yolo:
                                # Use our specialized ultralytics detector function for the smaller image
                                people = detect_people_ultralytics(model, small_img, confidence=Config.Model.CONFIDENCE)
                                skip_processing = True
                            else:
                                # Standard YOLOv5 inference on smaller image
                                results = model(small_img)
                                skip_processing = False
                                
                            OutputManager.log(f"Used scaled image ({small_img.shape[1]}x{small_img.shape[0]}) for detection", "INFO")
                        else:
                            raise e
                
                # Process results
                OutputManager.status("Processing detection results")
                
                # Skip processing if YOLOv8/v12 already handled
                if not ('skip_processing' in locals() and skip_processing):
                    # Handle different model result formats
                    try:
                        # Check if this is YOLOv8 or newer model (ultralytics model)
                        is_newer_yolo = False
                        for v in range(8, 20):  # Support YOLOv8 through YOLOv19
                            if model_name.lower().startswith(f"yolov{v}"):
                                is_newer_yolo = True
                                break
                        
                        # Skip processing if we already extracted people directly
                        if 'skip_processing' in locals() and skip_processing:
                            OutputManager.log("Skipping normal results processing, using direct extraction", "INFO")
                            OutputManager.log(f"People already detected: {len(people)}", "INFO")
                        elif is_newer_yolo:
                            # YOLOv8+ results format (including YOLOv12 and newer)
                            results_data = results
                            
                            # Using YOLOv8+ format (which is a different structure than YOLOv5)
                            if hasattr(results_data, 'boxes'):
                                # Ultralytics API format
                                for box in results_data[0].boxes:  # Access the first result's boxes
                                    cls = int(box.cls.item()) if hasattr(box, 'cls') else 0  # Default to person class if not found
                                    
                                    # Check if this is a person (class 0)
                                    if (len(Config.Model.CLASSES) == 0 or cls in Config.Model.CLASSES):
                                        conf = float(box.conf.item()) if hasattr(box, 'conf') else 0.0
                                        
                                        if conf >= Config.Model.CONFIDENCE:
                                            try:
                                                # Handle different box format possibilities
                                                if hasattr(box, 'xyxy'):
                                                    if hasattr(box.xyxy, 'cpu'):
                                                        xyxy = box.xyxy.cpu().numpy()[0]
                                                    else:
                                                        xyxy = box.xyxy[0].numpy() if hasattr(box.xyxy[0], 'numpy') else box.xyxy[0]
                                                elif hasattr(box, 'xywh'):
                                                    # Convert xywh to xyxy
                                                    xywh = box.xywh.cpu().numpy()[0] if hasattr(box.xywh, 'cpu') else box.xywh[0]
                                                    x, y, w, h = xywh
                                                    xyxy = [x-w/2, y-h/2, x+w/2, y+h/2]
                                                else:
                                                    # Try a generic approach for other formats
                                                    xyxy = box.cpu().numpy()[0] if hasattr(box, 'cpu') else box[0]
                                                
                                                x1, y1, x2, y2 = map(int, xyxy)
                                                
                                                # Calculate center point and foot position
                                                center_x = (x1 + x2) // 2
                                                center_y = (y1 + y2) // 2
                                                foot_x = center_x
                                                foot_y = y2  # Bottom of bounding box represents feet
                                                
                                                # Add to people list
                                                people.append({
                                                    'position': (center_x, center_y),
                                                    'foot_position': (foot_x, foot_y),
                                                    'bbox': (x1, y1, x2, y2),
                                                    'confidence': conf
                                                })
                                            except Exception as e:
                                                OutputManager.log(f"Error processing detection: {str(e)}", "ERROR")
                            # Check for different new result formats
                            elif hasattr(results_data, 'pred') and isinstance(results_data.pred, list):
                                # Direct prediction format (used in some newer models)
                                for det in results_data.pred[0]:
                                    if len(det) >= 6:  # xyxy, conf, cls
                                        x1, y1, x2, y2 = map(int, det[:4])
                                        conf = float(det[4])
                                        cls_id = int(det[5])
                                        
                                        if (len(Config.Model.CLASSES) == 0 or 
                                            cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                            
                                            # Calculate center point and foot position
                                            center_x = (x1 + x2) // 2
                                            center_y = (y1 + y2) // 2
                                            foot_x = center_x
                                            foot_y = y2  # Bottom of bounding box represents feet
                                            
                                            # Add to people list
                                            people.append({
                                                'position': (center_x, center_y),
                                                'foot_position': (foot_x, foot_y),
                                                'bbox': (x1, y1, x2, y2),
                                                'confidence': conf
                                            })
                            elif isinstance(results_data, list) and len(results_data) > 0:
                                # Alternative format sometimes returned by YOLOv8/YOLOv12
                                detections = None
                                
                                # Handle different tensor/numpy array formats
                                if hasattr(results_data[0], 'numpy'):
                                    detections = results_data[0].numpy()
                                elif hasattr(results_data[0], 'cpu'):
                                    detections = results_data[0].cpu().numpy()
                                else:
                                    detections = results_data[0]
                                    
                                if not isinstance(detections, list):
                                    # Make sure we iterate over rows
                                    if hasattr(detections, 'shape') and len(detections.shape) > 1:
                                        for det in detections:
                                            if len(det) >= 6:  # xyxy, conf, cls
                                                x1, y1, x2, y2 = map(int, det[:4])
                                                conf = float(det[4])
                                                cls_id = int(det[5])
                                                
                                                if (len(Config.Model.CLASSES) == 0 or 
                                                    cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                                    
                                                    # Add to people list
                                                    people.append({
                                                        'position': ((x1 + x2) // 2, (y1 + y2) // 2),
                                                        'foot_position': ((x1 + x2) // 2, y2),
                                                        'bbox': (x1, y1, x2, y2),
                                                        'confidence': conf
                                                    })
                                else:
                                    # Process list-format detections
                                    for det in detections:
                                        if len(det) >= 6:  # xyxy, conf, cls
                                            x1, y1, x2, y2 = map(int, det[:4])
                                            conf = float(det[4])
                                            cls_id = int(det[5])
                                            
                                            if (len(Config.Model.CLASSES) == 0 or 
                                                cls_id in Config.Model.CLASSES) and conf >= Config.Model.CONFIDENCE:
                                                
                                                # Calculate center point and foot position
                                                center_x = (x1 + x2) // 2
                                                center_y = (y1 + y2) // 2
                                                foot_x = center_x
                                                foot_y = y2  # Bottom of bounding box represents feet
                                                
                                                # Add to people list
                                                people.append({
                                                    'position': (center_x, center_y),
                                                    'foot_position': (foot_x, foot_y),
                                                    'bbox': (x1, y1, x2, y2),
                                                    'confidence': conf
                                                })
                        else:
                            # YOLOv5 results format - using pandas
                            df = results.pandas().xyxy[0]
                            df = df[df['class'] == 0]
                            
                            for _, row in df.iterrows():
                                x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
                                
                                # Calculate center point and foot position
                                center_x = (x1 + x2) // 2
                                center_y = (y1 + y2) // 2
                                foot_x = center_x
                                foot_y = y2  # Bottom of bounding box represents feet
                                
                                # Add to people list
                                people.append({
                                    'position': (center_x, center_y),
                                    'foot_position': (foot_x, foot_y),
                                    'bbox': (x1, y1, x2, y2),
                                    'confidence': row['confidence']
                                })
                    except Exception as e:
                        error_msg = str(e)
                        
                        # Add more specific error messages for YOLOv8+ issues
                        if "'list' object has no attribute 'pandas'" in error_msg:
                            # Extract the model version
                            version_match = re.search(r'yolov(\d+)', model_name.lower())
                            version = version_match.group(1) if version_match else "newer"
                            OutputManager.log(f"YOLOv{version} result format error. This model requires the ultralytics package.", "ERROR")
                            OutputManager.log("Install ultralytics with: pip install ultralytics", "INFO")
                            OutputManager.log("Or try YOLOv5 instead with: --model yolov5s", "INFO")
                        elif "no attribute 'numpy'" in error_msg or "tensor" in error_msg.lower():
                            OutputManager.log(f"YOLO tensor processing error with {model_name}.", "ERROR")
                            OutputManager.log("Install ultralytics with: pip install ultralytics", "INFO")
                        else:
                            OutputManager.log(f"Problem detecting people: {str(e)}", "ERROR")
                        
                        # Continue with empty people list
                        people = []
                    
                    # Report how many people we found
                    OutputManager.log(f"Found {len(people)} {'person' if len(people) == 1 else 'people'} in the image", "SUCCESS")
                    if Config.Output.EXTRA_VERBOSE and people:
                        # Log details of detected people
                        for i, person in enumerate(people):
                            x1, y1, x2, y2 = person['bbox']
                            conf = person['confidence']
                            OutputManager.log(f"Person {i+1}: bbox=({x1},{y1},{x2},{y2}), confidence={conf:.2f}", "DEBUG")
            except Exception as e:
                error_msg = str(e)
                
                # Handle different types of errors with specific messages
                if "model" in error_msg.lower() or "yolo" in error_msg.lower() or "no such file" in error_msg.lower():
                    OutputManager.log(f"YOLOv5 model not found: {error_msg}", "ERROR")
                    OutputManager.log("Model missing - run: mkdir -p models && wget -q https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt -O models/yolov5s.pt", "ERROR")
                elif "cuda" in error_msg.lower() or "gpu" in error_msg.lower():
                    OutputManager.log(f"CUDA error: {error_msg}", "ERROR")
                    OutputManager.log("CUDA error - run: pip install -r requirements.txt", "ERROR")
                elif "ssl" in error_msg.lower():
                    OutputManager.log(f"SSL error: {error_msg}", "ERROR")
                    OutputManager.log("SSL error - run: pip install certifi --upgrade", "ERROR")
                elif "memory" in error_msg.lower():
                    OutputManager.log(f"Memory error: {error_msg}", "ERROR")
                    OutputManager.log("Memory error - try using a smaller image or reducing batch size", "ERROR")
                elif "timeout" in error_msg.lower() or "connection" in error_msg.lower():
                    OutputManager.log(f"Network error: {error_msg}", "ERROR")
                    OutputManager.log("Network error - check your internet connection and try again", "ERROR")
                else:
                    OutputManager.log(f"Problem detecting people: {error_msg}", "ERROR")
                
                # Continue with empty people list
                people = []
            
            # Determine if each person is on a court
            people_locations = []
            try:
                if people and courts:
                    OutputManager.status("Analyzing positions using multiprocessing")
                    
                    # Process in parallel using our optimized function
                    people_locations = analyze_people_positions_parallel(people, courts)
                    
                    OutputManager.log("Positions analyzed successfully", "SUCCESS")
                    
                    # Count people by location type
                    in_bounds_count = sum(1 for _, area_type in people_locations if area_type == 'in_bounds')
                    out_bounds_count = sum(1 for _, area_type in people_locations if area_type == 'out_bounds')
                    off_court_count = sum(1 for _, area_type in people_locations if area_type == 'off_court')
                    
                    OutputManager.log(f"Position breakdown: {in_bounds_count} in-bounds, {out_bounds_count} sidelines, {off_court_count} off-court", "INFO")
                else:
                    # If no people or no courts, no need to analyze positions
                    for _ in range(len(people)):
                        people_locations.append((-1, 'off_court'))
            except Exception as e:
                OutputManager.log(f"Error analyzing positions: {str(e)}", "ERROR")
                # Create fallback position data
                for _ in range(len(people)):
                    people_locations.append((-1, 'off_court'))
            
            # Calculate court counts for summary
            court_counts = {}
            for court_idx, area_type in people_locations:
                if court_idx >= 0:
                    court_num = court_idx + 1
                    if court_num not in court_counts:
                        court_counts[court_num] = 0
                    court_counts[court_num] += 1
            
            # Log court counts
            for court_num in sorted(court_counts.keys()):
                OutputManager.log(f"Court {court_num}: {court_counts[court_num]} people", "INFO")
            
            # Create debug visualization showing foot positions on mask
            if debug_folder and Config.DEBUG_MODE and people:
                try:
                    debug_foot_positions = court_viz.copy()
                    for person_idx, person in enumerate(people):
                        if 'foot_position' in person:
                            foot_x, foot_y = person['foot_position']
                            # Draw foot position marker (circle)
                            cv2.circle(debug_foot_positions, (foot_x, foot_y), 10, (255, 255, 255), -1)
                            cv2.circle(debug_foot_positions, (foot_x, foot_y), 10, (0, 0, 0), 2)
                            # Label with person index
                            cv2.putText(debug_foot_positions, f"P{person_idx+1}", (foot_x+15, foot_y), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                    
                    cv2.imwrite(os.path.join(debug_folder, "foot_positions_debug.png"), debug_foot_positions)
                    OutputManager.log("Foot positions debug image saved", "DEBUG")
                except Exception as e:
                    OutputManager.log(f"Couldn't save foot positions debug image: {str(e)}", "WARNING")
            
            # Create final output image
            try:
                OutputManager.status("Creating output image")
                output_image = image.copy()
                
                # Draw court outlines with different colors
                for court in courts:
                    court_id = court['court_number']
                    color_idx = (court_id - 1) % len(court_colors)
                    court_color = court_colors[color_idx]
                    
                    # Extract court mask
                    court_mask_individual = (court_numbers_mask == court_id).astype(np.uint8) * 255
                    # Find contours of the court
                    court_contours, _ = cv2.findContours(court_mask_individual, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    
                    # Draw the court outline
                    cv2.drawContours(output_image, court_contours, -1, court_color, 2)
                    
                    # Draw court number at center only if enabled
                    if Config.Visual.SHOW_COURT_NUMBER:
                        cx, cy = int(court['centroid'][0]), int(court['centroid'][1])
                        cv2.putText(output_image, f"Court {court_id}", (cx-40, cy), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                
                # Draw people and their locations
                for i, person in enumerate(people):
                    court_idx, area_type = people_locations[i]
                    
                    # Draw bounding box and label
                    x1, y1, x2, y2 = person['bbox']
                    
                    # Choose color based on location
                    if court_idx >= 0:
                        court_number = court_idx + 1
                        if area_type == 'in_bounds':
                            color = Config.Visual.PERSON_IN_BOUNDS_COLOR
                            label = f"Court {court_number}" if Config.Visual.SHOW_DETAILED_LABELS else ""
                        else:  # out_bounds
                            color = Config.Visual.PERSON_OUT_BOUNDS_COLOR
                            label = f"Court {court_number} • Sideline" if Config.Visual.SHOW_DETAILED_LABELS else ""
                    else:
                        color = Config.Visual.PERSON_OFF_COURT_COLOR
                        label = "Not on court" if Config.Visual.SHOW_DETAILED_LABELS else ""
                    
                    # Draw bounding box
                    cv2.rectangle(output_image, (x1, y1), (x2, y2), color, 2)
                    
                    # Draw foot position marker - smaller and less intrusive
                    foot_x, foot_y = person['foot_position']
                    cv2.circle(output_image, (foot_x, foot_y), 3, color, -1)
                    
                    # Only draw text labels if specified
                    if Config.Visual.SHOW_DETAILED_LABELS and label:
                        # Draw label with black background for readability
                        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                                   Config.Visual.FONT_SCALE, 
                                                   Config.Visual.TEXT_THICKNESS)[0]
                        cv2.rectangle(output_image, (x1, y1 - text_size[1] - 5), 
                                     (x1 + text_size[0], y1), color, -1)
                        cv2.putText(output_image, label, (x1, y1 - 5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 
                                   Config.Visual.FONT_SCALE, 
                                   Config.Visual.TEXT_COLOR, 
                                   Config.Visual.TEXT_THICKNESS)
                        
                        # Add person index number
                        cv2.putText(output_image, f"Person {i+1}", (x1, y2 + 20), 
                                    cv2.FONT_HERSHEY_SIMPLEX, Config.Visual.FONT_SCALE, 
                                    color, Config.Visual.TEXT_THICKNESS)
                    else:
                        # Just add a small number indicator for simpler display
                        cv2.putText(output_image, f"{i+1}", (x1, y1 - 5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 
                                   Config.Visual.FONT_SCALE, 
                                   Config.Visual.TEXT_COLOR, 
                                   Config.Visual.TEXT_THICKNESS)
                
                OutputManager.log("Output image generated", "SUCCESS")
            except Exception as e:
                OutputManager.log(f"Error creating output image: {str(e)}", "ERROR")
                output_image = image.copy()  # Use original image as fallback
            
            # Save the final output image
            output_path = Config.Paths.output_path()
            try:
                OutputManager.status("Saving output image")
                
                # Ensure output directory exists
                output_dir = os.path.dirname(output_path)
                if not os.path.exists(output_dir):
                    try:
                        os.makedirs(output_dir, exist_ok=True)
                        OutputManager.log(f"Created output directory at {output_dir}", "INFO")
                    except Exception as e:
                        OutputManager.log(f"Cannot create output directory: {str(e)}", "ERROR")
                
                cv2.imwrite(output_path, output_image)
                OutputManager.log(f"Output image saved successfully to {output_path}", "SUCCESS")
            except Exception as e:
                OutputManager.log(f"Error saving output image: {str(e)}", "ERROR")
                output_path = None
            
            # Create the adaptive final summary
            processing_time = time.time() - start_time
            OutputManager.log(f"Total processing time: {processing_time:.2f} seconds", "INFO")
            
            final_summary = OutputManager.create_final_summary(
                people_count=len(people),
                court_counts=court_counts,
                output_path=output_path,
                total_courts=len(courts)  # Pass the total number of courts
            )
            
            # Use the fancy summary method
            OutputManager.fancy_summary(
                "RESULTS SUMMARY", 
                final_summary, 
                processing_time=processing_time
            )
            
            # If there were errors that didn't cause a fatal exit, still indicate an error status
            if OutputManager.errors:
                sys.exit(1)
            
            sys.exit(0)
        except Exception as e:
            # This is the main catch-all for any unhandled exceptions in the try block
            OutputManager.log(f"Unhandled error in main function: {str(e)}", "ERROR")
            
            # Create a basic summary with the error
            processing_time = time.time() - start_time
            final_summary = OutputManager.create_final_summary(
                people_count=None, 
                court_counts={}, 
                output_path=None,
                processing_time=processing_time,
                total_courts=0
            )
            print_error_summary(final_summary)
            sys.exit(1)
    except Exception as e:
        # For other unhandled exceptions, provide a generic error message
        error_message = str(e)
        possible_solution = ""
        possible_solution = "Check requirements with: pip install -r requirements.txt"
        
        print("\n" + "╭" + "─" * 78 + "╮")
        print("│ " + "ERROR: UNHANDLED EXCEPTION".center(78) + " │")
        print("│ " + "─" * 78 + " │")
        
        # Split long error messages
        wrapped_error = []
        for chunk in [error_message[i:i+78] for i in range(0, len(error_message), 78)]:
            wrapped_error.append(chunk)
        
        for line in wrapped_error[:3]:  # Limit to 3 lines to avoid huge error messages
            print("│ " + line.ljust(78) + " │")
        
        if len(wrapped_error) > 3:
            print("│ " + "...".ljust(78) + " │")
        
        print("│ " + "─" * 78 + " │")
        print("│ " + "POSSIBLE SOLUTION:".ljust(78) + " │")
        for sol_line in [possible_solution[i:i+78] for i in range(0, len(possible_solution), 78)]:
            print("│ " + sol_line.ljust(78) + " │")
        print("╰" + "─" * 78 + "╯")
        sys.exit(1)